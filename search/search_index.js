var __index = {"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/camelup.html","title":"Yo-Yo attacks on cloud auto-scaling","text":"<p>Camel Up is a popular board game designed by Steffen Bogen. It is a family-friendly, light-hearted, and entertaining game that revolves around a crazy camel racing event. What makes Camel Up particularly unique and enjoyable is the unpredictability of the race. The camels stack on top of each other when they land on the same space, creating a camel stack. The camels at the top of the stack move faster, while the ones at the bottom move slower. This adds an element of surprise and excitement, making it challenging to predict the winner until the very end.  </p> <p>Camel Up board game and auto-scaling in the cloud may appear as two completely distinct area. However, the camels' movement\u2014going up and down\u2014in the game reminds me of the process of scaling resources in cloud auto-scaling. </p> <p>Cloud auto-scaling is a very powerful tool, but it can also be a double-edged sword. Without the proper scaling configuration and testing it can cost cloud users a lot. So, auto-scaling is a trade of performance and cost.</p>","tags":["Cloud"]},{"location":"blog/camelup.html#story","title":"Story","text":"<p>In this blog, the term <code>scale up/down</code> refers to scaling resources, encompassing both vertical and/or horizontal scaling.</p> <p>Over the past decades, DoS and DDoS attacks have emerged as a grave threat to the Internet's infrastructure. Recent data from Cloudflare indicates a dramatic surge in DDoS attacks over the past few years [comparitech.com]. The increasing number of attacks has led to the emergence of new detection and mitigation solutions as well. In November 2020, the Alibaba Cloud Security Team detected the largest resources exhaustion DDoS attack on their cloud platform, with a peak of 5.369 million QPS (Queries Per Second). Microsoft mitigated upwards of 359,713 DDoS attacks against their Azure Cloud infrastructures during the second half of 2021 [microsoft.com].</p> <p>On the other hand, attackers do not surrender. New kinds of DDoS attacks have emerged to exploit cloud anti-DDoS solutions. Burst attacks, also known as hit-and-run DDoS, are a new kind of DDoS attack where the attacker launches periodic bursts of traffic overload at random intervals on online targets. Burst attacks have grown significantly, to the extent that a comprehensive survey in 2017 revealed that half of the participants cited an increase in burst attacks. [radware.com]. </p> <p>Enterprises using cloud services mostly benefit from cloud features. Enhanced cloud scalability and elasticity through auto-scaling allow customers to dynamically scale their applications. Incoming traffic is distributed evenly across multiple endpoints, so individual backend services cannot be overwhelmed until the volume of traffic approaches the capacity of the entire network.  </p> <p>Hostile actors adjust their tactics to correspond to the realities posed by the cloud. The Yo-Yo attack is a new attack technique against the cloud autoscaling feature. In this method, attackers send a burst of request traffic to significantly increase the load on the cloud server, triggering the autoscaling mechanism, and causing the server to be scaled up. </p> <p>Therefore, during this confirmation period, the victim's system deploys resources that far exceed the required amount. Burst traffic will be stopped after scaling up and waiting for the auto-scaling mechanism to scale down the server. The attacker continues the latter attack procedure and forces the cloud services to scale up and scale down continuously. This adds extra load to the services to respond to the fake requests. In effect, the attacker forces the victim to pay for large amounts of resources that are not actually necessary to handle the legitimate workload. The Yo-Yo attack can affect any platform using auto-scaling mechanisms, such as container-based and Kubernetes platforms.</p>","tags":["Cloud"]},{"location":"blog/camelup.html#yo-yo-attack","title":"Yo-Yo attack","text":"<p>The Yo-Yo attack is designed to exploit the cloud's auto-scaling mechanism. The attacker employs a specific strategy to create a significant load on the cloud by sending bursts of request traffic to a target running on the cloud. As a result, the auto-scaling mechanism triggers, attempting to scale up the cloud resources to handle the high traffic load.</p> <p>The attack operates in a cyclical manner. After the attacker notices the scaling-up process, they halt the burst attack and wait for the auto-scaling mechanism to scale down the resources. This crucial step is key to the success of the Yo-Yo attack. The attacker then resumes sending the burst traffic to trigger the auto-scaling mechanism to scale up again, perpetuating the cycle.</p> <p>The primary objective of the Yo-Yo attack is not necessarily to take the services offline but rather to inflict financial damage on the victim. Cloud service providers typically follow a consumption-based pricing model, wherein end-users pay for the resources they utilize. This model allows users to pay for additional resources as needed and stop paying for resources that are no longer required. The Yo-Yo attack aims to exploit this pricing model by forcing the victim's system to scale up and consume more resources during the attack cycles, leading to increased costs for the victim without actually providing any legitimate workload.</p> <p>During each cycle of the Yo-Yo attack, as the victim's system scales in resources, significant charges accumulate on the victim's account. The attacker forces the victim to pay for large amounts of resources that are not truly necessary to handle the current legitimate workload. For example, cloud providers like AWS charge for EC2 instances based on the time they are consumed, and partial instance-hours are billed per second. This means that after scaling in an instance, the service provider incurs charges per second, even if the scale down occurred due to a Yo-Yo attack. Consequently, the financial damage to the victim can be substantial, as they are billed for the extra resources forced upon them by the attacker during each cycle of the attack.</p> <p>The graph below shows the results of simulating a Yo-Yo attack towards AWS EC2 instances running on autoscaling groups. Please keep in mind that the test has been conducted under the AWS DDoS simulation testing policy.</p> <p></p>","tags":["Cloud"]},{"location":"blog/camelup.html#related-studies","title":"Related studies","text":"<p>The Yo-Yo attack is a new DDoS technique that has emerged due to the cloud auto-scaling feature. So, the number of studies in this area is very limited. In a paper published in 2017, Anat Bremler-Barr, Eli Brosh, and Mor Sides were the first to discuss the Yo-Yo attack on the autoscaling mechanism. This groundbreaking paper demonstrates that, apart from the economic effects of the attack, the Yo-Yo attack can inflict substantial performance damage.</p> <p>During the repetitive scale up process, which takes several minutes due to the instance startup process, the cloud service suffers from significant performance degradation. The article reveals that the autoscaling policy configuration is an important factor in minimizing the impact of the Yo-Yo attack. Therefore, the Yo-Yo attack can also be classified as a type of Reduction of Quality (RoQ) attack.</p> <p>Based on this study, another group published a paper with the topic 'Towards Yo-Yo attack mitigation in cloud auto-scaling mechanisms'. The paper proposed a detection and mitigation system for Yo-Yo attacks in cloud auto-scaling mechanisms. The suggested approach is called Trust-based Adversarial Scanner Delaying (<code>TASD</code>). The <code>TASD</code> approach is inspired by two key factors. Firstly, in comparison to benign users, Yo-Yo attackers tend to initiate burst requests, leading to more frequent auto-scaling. Additionally, there is a substantial difference in request load between the scale up and scale down phases caused by the attackers. To address this, the <code>TASD</code> system assigns a trust value to each client based on their behavior, which represents their Quality of Service (QoS). Consequently, <code>TASD</code> introduces specific delays to suspicious requests within the QoS constraints. This manipulation of response times aims to deceive the attackers and mitigate the impact of the Yo-Yo attack.</p> <p>During my master's thesis, I implemented a realistic scenario to test the <code>TASD</code> approach and enhance the mitigation algorithm. In the original <code>TASD</code> system, an Additive Decrease method was used to update the trust value. To improve the system, I drew inspiration from TCP rate control mechanisms and introduced two optimization methods: <code>ADAI</code> (Additive Decrease/Additive Increase) and <code>MDAI</code> (Multiplicative Decrease/Additive Increase). These methods aim to optimize the <code>TASD</code> detection and mitigation system further.  I published the results in a paper that can be accessed here. </p>","tags":["Cloud"]},{"location":"blog/camelup.html#results","title":"Results","text":"<p>The study mainly focused on the difference between DDoS and Yo-Yo attacks on cloud autoscaling and implemented mitigation and detection methods. Surprisingly, I have found some interesting side results:</p> <ol> <li> <p>The warming time of scale up is the duration it takes for an instance (VM, Container, etc.) to get ready to function, while the warming time of scale down is the duration an instance allocates to close all services and release resources. The warming time plays a significant role in the damage, especially with a simple auto-scaling policy. Whenever the scaling metric threshold was selected close to the maximum capacity of the service (e.g., web application), the service degraded during scale up due to the warming time.</p> <p>To address this issue, several approaches can be considered. One approach is to minimize the warming time, but this may not be feasible for all scenarios, as some services require time to initialize properly. Another approach is to scale up in two steps, where the auto-scaling increases the number of instances by two. However, a potential bottleneck with this option is the need to keep some unused capacity available for a quick response, which can result in ongoing costs for the user.</p> <p>An alternative option is to adopt an early scale up strategy and a slow scale down approach, allowing ample time before immediately scaling in the instances.</p> </li> <li> <p>Yo-Yo attackers should approximate the auto-scaling state and configuration to maximize the damage. The attacker can send probe requests and compare response times to detect the scale up status. The same technique is suggested to detect scale down. However, the experiment's results revealed that using a probe request may not necessarily detect the scaling status accurately. For example, in my test scenario, the auto-scaling policy was configured to scale based on the<code>Request Count</code>, resulting in stable response times for the probe requests during the test. This stability prevented the attacker from effectively detecting the scaling process.</p> </li> </ol>","tags":["Cloud"]},{"location":"blog/coming-soon.html","title":"The blog will be posted soon!","text":""},{"location":"blog/gizmos.html","title":"Take an action based on AWS API calls","text":"<p>Don\u2019t lose your energy marbles in this crazy building game. Gizmos puts players in the role of inventors. By using the four types of energy marbles, they will create their own personal Gizmo, adding on one Machine card at a time. Machines award you victory points and enable additional actions upon meeting specific conditions. As you build, new attachments can trigger chain reactions, letting you do even more on your turn. For more information, visit: boardgamegeek </p>","tags":["AWS","Terraform"]},{"location":"blog/gizmos.html#overview-detect-aws-console-login","title":"Overview; Detect AWS console login","text":"<p>This blog post explores the integration of AWS CloudTrail and EventBridge, forming a <code>winning combination</code>. The objective is to detect and notify logins to my AWS account. So, whenever someone successfully logs into my AWS account, I will receive an email notification containing the login details.</p> <p>While this blog post may seem straightforward, it serves as an insightful initiative for developing more intricate integrations. Towards the end of the post, I will present some additional ideas. Before diving into the implementation, let's first explore the concept of event-driven architecture. After gaining a solid understanding of this concept, we'll proceed to examine AWS EventBridge and CloudTrail.</p>","tags":["AWS","Terraform"]},{"location":"blog/gizmos.html#event-driven-architecture","title":"Event-Driven Architecture","text":"<p>Event-driven architecture relies on events to initiate communication among loosely connected services, a common feature in modern microservices-based applications. But what exactly constitutes an event? An event can be defined as a change in state or an update, for example in this blog post example, login status. Another example could be altering the state of an EC2 machine. Or consider an e-commerce application, where events include actions like adding or removing items from the shopping cart. </p> <p>Within this architecture, events are generated by a producer, and actions are taken by a consumer in response to these events. The system may feature multiple consumers, necessitating the routing of events from the producer to these consumers. Event brokers play a crucial role in facilitating the seamless flow of events between producers and consumers.</p> <p></p> <p>Let's delve into the various roles in this example. Returning to the project's objective, I aim to identify user console logins and receive an email for each successful login. In this architecture, my intention is to leverage native AWS services for login detection and email notifications (<code>Zero-Code</code> approach). The following section provides a brief overview of each party involved.</p>","tags":["AWS","Terraform"]},{"location":"blog/gizmos.html#aws-cloudtrail","title":"AWS CloudTrail","text":"<p>AWS CloudTrail is a service that allows users to monitor, log, and retain account activity within their AWS infrastructure. This service captures detailed information about actions taken through the AWS Management Console, AWS Command Line Interface (CLI), AWS SDKs, and other AWS services. By providing a transparent and comprehensive trail of events, AWS CloudTrail enhances security and compliance by enabling users to track changes, detect potential security threats, and troubleshoot operational issues. With its ability to deliver a detailed chronological record of API calls and related events, AWS CloudTrail is a valuable tool to detect user console login. According to the event-driven architecture, AWS CloudTrail acts as an event producer.</p>","tags":["AWS","Terraform"]},{"location":"blog/gizmos.html#aws-sns","title":"AWS SNS","text":"<p>Amazon Simple Notification Service (Amazon SNS) is a fully managed messaging service to facilitate the building of distributed, scalable, and reliable applications by enabling the decoupling of microservices, systems, and distributed components. With SNS, users can send messages, or notifications, to a distributed set of subscribers or endpoints via various protocols, including HTTP, HTTPS, email, SMS, and more. AWS SNS is widely used for building event-driven systems, managing workflows, and sending alerts in a scalable and efficient manner within the AWS cloud environment. I utilize AWS SNS to send notifications based on the events received from AWS CloudTrail.</p>","tags":["AWS","Terraform"]},{"location":"blog/gizmos.html#aws-eventbridge","title":"AWS EventBridge","text":"<p>And finally, there's AWS EventBridge, which serves as the heart of this integration. Amazon EventBridge is a serverless event bus service designed to connect various applications and AWS services seamlessly. It plays a crucial role in decoupling the architecture and simplifying event routing. I highly recommend checking out the AWS tutorial on AWS EventBridge for more in-depth insights: AWS EventBridge Learning </p> <p>Now, let's enhance the event-driven architecture diagram to reflect the changes made in this project:  </p> <p></p>","tags":["AWS","Terraform"]},{"location":"blog/gizmos.html#implementation","title":"Implementation","text":"<p>Finally, I've reached the point of detailing the implementation steps, which happens to be the most straightforward section for me to write. As indicated by the weblog's title, the implementation is zero-code, eliminating the necessity for any custom scripting. I opt for Terraform for the entire deployment process. However, if Terraform isn't your preference, you can effortlessly utilize the AWS console instead.</p> <p>The below diagram shows all the services utilized in this project:</p> <p></p> <p>Firstly, we need to enable AWS CloudTrail, and for this, an AWS S3 bucket is required to store all the logs. Therefore, the initial step involves creating a bucket:</p> <pre><code># data blocks gather information\ndata \"aws_caller_identity\" \"gizmos\" {}\n\ndata \"aws_partition\" \"gizmos\" {}\n\ndata \"aws_region\" \"gizmos\" {}\n\n# data block of S3 bucket iam policy document\ndata \"aws_iam_policy_document\" \"gizmos\" {\n  statement {\n    sid    = \"AWSCloudTrailAclCheck\"\n    effect = \"Allow\"\n\n    principals {\n      type        = \"Service\"\n      identifiers = [\"cloudtrail.amazonaws.com\"]\n    }\n\n    actions   = [\"s3:GetBucketAcl\"]\n    resources = [aws_s3_bucket.gizmos.arn]\n    condition {\n      test     = \"StringEquals\"\n      variable = \"aws:SourceArn\"\n      values   = [\"arn:${data.aws_partition.gizmos.partition}:cloudtrail:${data.aws_region.gizmos.name}:${data.aws_caller_identity.gizmos.account_id}:trail/cloudynotes-gizmos-trial\"]\n    }\n  }\n\n  statement {\n    sid    = \"AWSCloudTrailWrite\"\n    effect = \"Allow\"\n\n    principals {\n      type        = \"Service\"\n      identifiers = [\"cloudtrail.amazonaws.com\"]\n    }\n\n    actions   = [\"s3:PutObject\"]\n    resources = [\"${aws_s3_bucket.gizmos.arn}/AWSLogs/${data.aws_caller_identity.gizmos.account_id}/*\"]\n\n    condition {\n      test     = \"StringEquals\"\n      variable = \"s3:x-amz-acl\"\n      values   = [\"bucket-owner-full-control\"]\n    }\n    condition {\n      test     = \"StringEquals\"\n      variable = \"aws:SourceArn\"\n      values   = [\"arn:${data.aws_partition.gizmos.partition}:cloudtrail:${data.aws_region.gizmos.name}:${data.aws_caller_identity.gizmos.account_id}:trail/cloudynotes-gizmos-trial\"]\n    }\n  }\n}\n\n# Create S3 bucket\nresource \"aws_s3_bucket\" \"gizmos\" {\n  bucket        = \"cloudynotes-gizmos-trial\"\n  force_destroy = true\n}\n\n# Add S3 bucket policy\nresource \"aws_s3_bucket_policy\" \"gizmos\" {\n  bucket = aws_s3_bucket.gizmos.id\n  policy = data.aws_iam_policy_document.gizmos.json\n}\n</code></pre> <p>And then we can enable AWS CloudTrail:</p> <pre><code># Enable AWS CloudTrial\nresource \"aws_cloudtrail\" \"gizmos\" {\n  depends_on = [aws_s3_bucket_policy.gizmos]\n\n  name                          = \"cloudynotes-gizmos-trial\"\n  s3_bucket_name                = aws_s3_bucket.gizmos.id\n  include_global_service_events = true\n  is_multi_region_trail         = true\n}\n</code></pre> <p>Note - For a multi-region trail, this resource must be in the home region of the trail. - Console login log needs capturing events from IAM, so <code>include_global_service_events</code> must be enabled.  </p> <p>With the producer configuration complete, the next step involves creating the consumer, which is an SNS topic. Utilize the code blocks below to create an SNS topic for sending emails upon receiving an event:</p> <pre><code># Create AWS SNS topic\nresource \"aws_sns_topic\" \"gizmos_sns_topic\" {\n  name         = \"gizmos-aws-sign-in\"\n  display_name = \"Gizmos login\"\n}\n\n# Add SNS policy\nresource \"aws_sns_topic_policy\" \"gizmos_topic_policy\" {\n  arn    = aws_sns_topic.gizmos_sns_topic.arn\n  policy = data.aws_iam_policy_document.gizmos_topic_policy.json\n}\n\n# data block of SNS policy document\ndata \"aws_iam_policy_document\" \"gizmos_topic_policy\" {\n  statement {\n    effect  = \"Allow\"\n    actions = [\"SNS:Publish\"]\n\n    principals {\n      type        = \"Service\"\n      identifiers = [\"events.amazonaws.com\"]\n    }\n\n    resources = [aws_sns_topic.gizmos_sns_topic.arn]\n  }\n}\n</code></pre> <p>Now add an email topic subscription: </p> <pre><code>resource \"aws_sns_topic_subscription\" \"gizmos_sqs_subscription\" {\n  topic_arn = aws_sns_topic.gizmos_sns_topic.arn\n  protocol  = \"email\"\n  endpoint  = var.subscription_email\n}\n</code></pre> <p>You can add <code>subscription_email</code> to the Terraform varaibles or simply replace your email address in the above block: endpoint = \"gizmos@cloudynotes.io\"</p> <p>The important and final step is to set up AWS EventBridge to capture events from AWS CloudTrail and send them to AWS SNS. Use the below code block to create event rule:</p> <pre><code>resource \"aws_cloudwatch_event_rule\" \"gizmos_event_rule\" {\n  name        = \"gizmos-aws-sign-in\"\n  description = \"Capture each AWS Console Sign In\"\n\n  event_pattern = &lt;&lt;PATTERN\n  {\n  \"source\": [\"aws.signin\"],\n  \"detail-type\": [\"AWS Console Sign In via CloudTrail\"],\n  \"detail\": {\n    \"eventSource\": [\"signin.amazonaws.com\"],\n    \"eventName\": [\"ConsoleLogin\"]\n    }\n  }\n  PATTERN\n}\n</code></pre> <p>As you can see the Terraform resource is CloudWatch event rule, EventBridge was formerly known as CloudWatch Events. The functionality is identical.</p> <p>A helpful tip for finding the correct pattern for your event is to utilize the AWS EventBridge console. This tool assists in generating a sample event, allowing you to test your pattern before creating your rule. Navigate to your AWS Console, go to the EventBridge service, and select \"Rule\" from the left side to create a sample rule. On the next page, choose \"AWS events\" as sample events and then filter your sample event. In this example, you might filter for \"AWS Console Sign In via CloudTrail\":</p> <p></p> <p>Then, at the bottom of the page, you can select your method and define your event pattern.</p> <p></p> <p>We are approaching the final step of the configuration; adding a target to the event rule. Since we have created an SNS topic as the target, we need to include it in the event rule target. The event rule includes an excellent feature that allows for the transformation of input data, enabling the sending of custom event data to the target. This functionality proves valuable in eliminating unnecessary information from the event or customizing the event data. Read more about it here.</p> <pre><code>resource \"aws_cloudwatch_event_target\" \"gizmos_event_target\" {\n  rule      = aws_cloudwatch_event_rule.gizmos_event_rule.name\n  target_id = \"gizmosLoginSNS\"\n  arn       = aws_sns_topic.gizmos_sns_topic.arn\n\n  input_transformer {\n    input_paths = {\n      aws_account = \"$.detail.awsRegion\",\n      time        = \"$.time\",\n      user        = \"$.detail.userIdentity.accountId\"\n    }\n    input_template = &lt;&lt;EOF\n        \"Warning! Successfully loggin to &lt;aws_account&gt; at &lt;time&gt; with user: &lt;user&gt;!\"\n    EOF\n  }\n}\n</code></pre> <p>Apply the Terraform resources and then you can test the setup by logout and loggin to your AWS account:</p> <p></p>","tags":["AWS","Terraform"]},{"location":"blog/gizmos.html#next-step","title":"Next step","text":"<p>Utilize this winning combination to implement various integrations and effortlessly solve problems within your projects. For instance, I discovered the following article that demonstrates using the same solution to detect a stopped EC2 instance and initiate its restart using Terraform. Read more about this solution in this blog post: Automating EC2 Stop Protection </p> <p></p>","tags":["AWS","Terraform"]},{"location":"blog/lambda-loop.html","title":"Unraveling Recursive Loops in AWS Lambda","text":"<p>The Next Station board game series begins with a London edition, and the latest addition features Tokyo. This game tasks players with redesigning the city's underground subway system. The goal is to optimize connections, serve as many attractions as possible, and make efficient use of the tunnels under the city. One of the fundamental rule of the game is that players cannot circle back to the starting station \u2013 in other words, loop is not permitted. For more information, visit: boardgamegeek </p> <p>Last year, I awoke to a startling email from AWS billing service. My personal AWS account had amassed charges amounting to $4,000. Initially, I dismissed it as a bizarre dream, a shepherd's nightmare, if you will. But reality struck\u2014it was a genuine email. The AWS billing page echoed the same staggering figure.</p> <p></p> <p>My AWS account is typically a sandbox for experimentation, with monthly expenses hovering around $100. So, what triggered this financial avalanche?</p> <p>A closer inspection of the bill pinpointed an unexpected surge in costs associated with AWS S3 (Simple Storage Service) and AWS Lambda. It dawned on me\u2014the culprit was my recent Lambda experiment. In an effort to automate object processing in a bucket, adding a watermark and re-storing, I had overlooked the need to dismantle my setup. A simple oversight, yet it nearly led to my financial undoing.</p> <p>Upon further investigation, the root of the problem became clear: an infinite loop. An S3 trigger was continuously invoking the Lambda function, which, in turn, saved the modified object back into the same bucket\u2014a relentless cycle akin to an endless subway line with no stations.</p> <p>The purpose of this post is to illuminate the perilous potential of infinite loops within Lambda functions and to share strategies for averting, detecting, and halting such cycles before they spiral out of control.</p>","tags":["Cloud","AWS"]},{"location":"blog/lambda-loop.html#invoking-lambda-and-recursive-loops","title":"Invoking Lambda and recursive loops","text":"<p>AWS Lambda function is an event-driven compute service that runs the code when certain events occur. You can invoke Lambda functions directly using the Lambda console, a function URL HTTP(S) endpoint, the Lambda API, an AWS SDK, the AWS Command Line Interface (AWS CLI), and AWS toolkits. You can also configure other AWS services to invoke your function in response to events or external requests, or on a schedule. For another AWS service to invoke your function directly, you need to create a trigger for your Lambda. </p> <p>So, a trigger is a resource you configure to allow another AWS service to invoke your function when certain events or conditions occur. Your function can have multiple triggers. Each trigger acts as a client invoking your function independently, and each event that Lambda passes to your function has data from only one trigger. You can find a list of AWS services that can be used as Lambda triggers here: Lambda invokers</p> <p>For example, when an item is added to an Amazon SQS queue or Amazon Simple Notification Service (Amazon SNS) topic. Lambda passes events to your function as JSON objects, which contain information about the change in the system state. When an event causes your function to run, this is called an invocation.</p> <p>An AWS Lambda recursive loop is an error that occurs when a Lambda function inadvertently calls itself repeatedly without an exit condition, leading to a potentially endless cycle of function invocations. </p> <p>Let's looking back to my infinitive subway lines with no station. Lambda recursive loop created by wrongly invoking the lambda function using S3 object put which then invokes the same function. This invocation causes the function to write another object into the same bucket, which in turn invokes the function again. </p> <p>Another example would be an Amazon SQS queue invoking your Lambda function. Your Lambda function would then send the processed event back to the same Amazon SQS queue, which would in turn invoke your function again.</p>","tags":["Cloud","AWS"]},{"location":"blog/lambda-loop.html#aws-built-in-recursive-loop-detection-method","title":"AWS built-in recursive loop detection method","text":"<p>In Jul 13, 2023 AWS introduced a new feature to detect and stop recursive loops in lambda functions for certain supported AWS services and SDKs. </p> <p>Lambda uses an AWS X-Ray trace header primitive called <code>Lineage</code> to track the number of times a function has been invoked with an event. (You do not need to configure active X-Ray tracing for this feature to work.) If your function is invoked more than 16 times in the same chain of requests, then Lambda automatically stops the next function invocation in that request chain and notifies you. If your function is configured with multiple triggers, then invocations from other triggers aren't affected.</p> <p>At the time of writing this blog post, the feature detects recurisve loops between Lambda function and Amazon SQS, and Amazon SNS. Also it detects loops in Lambda functions, which may invoke each other synchronously or asynchronously.</p> <p>Also the featute only works if the lambda function code using one of the below SDKs: - Node.js - Python - Java 8, Java 11 and Java 17 - NET - Ruby  </p> <p>If your design intentionally uses recursive patterns, then you can raise a AWS support request to turn off Lambda recursive loop detection.</p> <p>Find more information here: Lambda recursive loop detection</p>","tags":["Cloud","AWS"]},{"location":"blog/lambda-loop.html#custom-recursive-loop-detection-methods","title":"Custom recursive loop detection methods","text":"<p>As we can observe, there are still some services that could potentially trigger an infinite loop, which AWS has yet to provide detection support for. The problem I faced falls into one of these scenarios.</p> <p>In the scenario where a Lambda function is triggered by an S3 put event, it writes an object back to the S3 bucket. This action, in turn, triggers the same Lambda function again, creating a recursive loop of invocations and writings.</p> <p></p> <p>Below, I have gathered some possible approaches to prevent recursive loops when using AWS Lambda and S3:</p>","tags":["Cloud","AWS"]},{"location":"blog/lambda-loop.html#vary-source-and-destination-locations","title":"Vary source and destination locations","text":"<p>In my endless subway project, I determined that the most effective way to prevent this issue was by using two separate S3 buckets: one for reading the original objects and another for storing the watermarked objects. By directing the output objects to a different bucket, I eliminated the risk of triggering additional events from the source bucket, thereby avoiding the recursive loop. </p> <p>There are some scenarios that the object should write in the source bucket. The next solution describe some of them. (Ref: AWS blog)</p> <p></p>","tags":["Cloud","AWS"]},{"location":"blog/lambda-loop.html#event-source-filtering","title":"Event source filtering","text":"<p>Use S3 event source filtering to trigger the Lambda function only for objects that match certain prefix and/or suffix criteria. You can use naming convention to set a specific naming convention for files that should trigger the Lambda function. When the Lambda function processes a file, it can rename the file or move it to a different folder that does not trigger the function.</p> <p></p> <p>Check the below function trigger configuration:</p> <p></p>","tags":["Cloud","AWS"]},{"location":"blog/lambda-loop.html#add-metadata-tagging","title":"Add metadata tagging","text":"<p>The other solutin is to use metadata tagging. Add specific metadata to the S3 object after processing it. The Lambda function can check this metadata before processing to determine if the object has already been processed.</p> <p></p> <p>Here is a sample Python code that can be used in a function to check object metadata:</p> <pre><code>import boto3\nimport json\ns3 = boto3.client('s3')\n\ndef lambda_handler(event, context):\n  for record in event['Records']\n  bucket_name = record['s3']['bucket']['name']\n  object_key = record['s3']['object']['key']\n\n  # Get the object and its metadata from S3\n  response = s3.get_object(Bucket=bucket_name, Key=object_key)\n\n  # Check for the 'original' metadata\n  metadata = response.get('Metadata', {})\n  if metadata.get('original') == 'true':\n      file_content = response['Body'].read().decode('utf-8')\n      # Other functions\n\n  return {\n      'statusCode': 200,\n      'body': json.dumps('File processed successfully!')\n  }\n</code></pre> <p>In this solution, the Lambda function is always invoked twice for each uploaded S3 object.</p> <p>The blog post also suggested another solution using an AWS DynamoDB to store item keys, and then DynamoDB stream triggers another Lambda function to process objects.It writes the object back to the same source bucket. Because the same item is put to the DynamoDB table, this does not trigger a new DynamoDB stream event.</p> <p>Furthermore, building on the above solutions to prevent recursive loops in Lambda functions, I recommend some preventive measures.</p> <p>It is evident that programming mistakes or improper use of the service might lead to the Lambda function being triggered endlessly. </p>","tags":["Cloud","AWS"]},{"location":"blog/lambda-loop.html#using-an-aws-sqs-as-a-middleware","title":"Using an AWS SQS as a middleware","text":"<p>Since AWS Lambda's built-in recursive loop supports invocation through AWS SQS, we can utilize AWS SQS to trigger a Lambda function, which in turn reads an object from S3. This approach effectively prevents unintentional recursive loops.</p> <p></p>","tags":["Cloud","AWS"]},{"location":"blog/lambda-loop.html#lambda-function-permissions","title":"Lambda function permissions","text":"<p>Restrict your Lambda function\u2019s permissions so it can only read from or write to specific S3 paths. This can prevent it from accidentally processing unintended files.</p> <p>for example:  </p> <pre><code>\"arn:aws:s3:::${SourceBucket}/${SourcePrefix}/*\"\n\"arn:aws:s3:::${SourceBucket}/${SourcePrefix}\"\n</code></pre> <p>You can find more scenarios and details about possible recursive loops here: Recursive patterns that cause run-away Lambda functions</p> <p>And finally, if you are curious about my billing saga, I should mention that I reached out to AWS support. The team meticulously reviewed my case, and since my account wasn't a business one, they graciously absolved me of the debt.</p>","tags":["Cloud","AWS"]},{"location":"blog/main.html","title":"blogs","text":"<p>This blog section is a place where we can explore a wide range of topics and ideas, from the latest technology trends to tips for self-improvement and personal growth. Whether you're a tech enthusiast, a curious learner, or someone seeking inspiration, there's something for everyone here.</p> <p>{{ blog_content }}</p> <p>Thank you for joining me on this adventure, and I look forward to connecting with you soon!  </p>"},{"location":"blog/rulebook.html","title":"Unlocking the Power of Effective Documentation","text":"<p>Good documentation for a product is like a well-written rulebook in a boardgame - it makes the game easy to understand, enjoyable to play, and accessible to all.</p> <p>Root is a strategy board game that took the tabletop world by storm upon its release in 2018. Developed by Cole Wehrle and published by Leder Games, Root is a game of woodland might and right that pits players against each other in a battle for control over a richly detailed and immersive woodland kingdom. With its unique asymmetric gameplay, gorgeous artwork, and rich thematic setting, Root offers endless hours of fun and strategic challenge for players of all skill levels. Whether you're a seasoned tabletop enthusiast or a newcomer to the world of board games, Root is sure to captivate your imagination and test your strategic abilities to their limits. Although Root is a complex game with a rating of 3.78/5, a well-designed rulebook makes it easier to learn and start playing. For more information, please visit: BoardGameGeek</p>","tags":["General"]},{"location":"blog/rulebook.html#introduction","title":"Introduction","text":"<p>While a board game may be well-designed and not overly complex, it can be difficult to understand and enjoy without a good rulebook. Developers often focus on writing efficient, high-quality code that meets the requirements of the project. However, the importance of good documentation is sometimes overlooked. Good documentation is an essential component of any product, as it provides a roadmap for users to effectively understand and use it.  </p> <p>Recently, I had a discussion with a colleague about improving the technical documentation for our project, and he suggested checking out divio. The website, written by Daniele Procida, is based on <code>The Grand Unified Theory of Documentation</code> by David Laing, a popular and transformative documentation authoring framework. I have read the websites' articles and watched a YouTube video presented by Daniele. In this blog post, I will provide a summary of the suggested framework, as well as share my personal experience about effective documentation.    </p> <p>It doesn\u2019t matter how good your product is, because if its documentation is not good enough, people will not use it.  documentation.divio.com </p> <p>According to <code>the Grand Unified Theory of Documentation</code> framework, there are four distinct components of documentation, rather than one monolithic entity:  </p> <ul> <li>Tutorials  </li> <li>How-to guide  </li> <li>Reference materials  </li> <li>Discussions  </li> </ul> <p>By recognizing and addressing each of these components separately, we can create more effective and targeted documentation that meets the needs of both authors and readers.  </p>","tags":["General"]},{"location":"blog/rulebook.html#tutorials","title":"Tutorials","text":"<p>Tutorials are instructional lessons that guide readers through a series of steps to complete a project. They are designed to teach how to do something, rather than just to provide information. As a product owner, it is important to ensure that the end goal of the tutorial is both meaningful and achievable for a beginner user. A well-designed tutorial can help learners make sense of the product, while a poorly executed or missing tutorial can hinder the acquisition of new users. Writing and maintaining tutorials can be time-consuming, but they are essential for helping users to successfully navigate and utilize a project. </p> <p>The list below shows the main points of tutorial documents:    </p> <ul> <li>Writing should be clear and easy to understand for the reader.  </li> <li>Provide a hands-on experience that inspires the reader.  </li> <li>Start with simple concepts and gradually move towards more complex ones.  </li> <li>Make the document enjoyable to read. Avoid starting with complex information that may discourage the reader.  </li> <li>Learning by doing should be emphasized throughout the tutorial.  </li> <li>Ensure the tutorial is repeatable on all platforms, even if it requires more work.  </li> <li>Use concrete examples and specifics rather than generalizations.  </li> <li>The tutorial should focus on practical skills.  </li> <li>Avoid distractions and unnecessary options.  </li> <li>Emphasize the importance of the tutorial for those who are new to the project.  </li> <li>Demonstrate to newcomers that it is possible to learn and that the tutorial is oriented towards learning.  </li> </ul>","tags":["General"]},{"location":"blog/rulebook.html#how-to-guide","title":"How-to guide","text":"<p>The purpose of how-to guides is to provide a step-by-step process for solving real-world problems. They are goal-oriented and provide specific instructions for achieving a particular outcome. For example, how to setup your company VPN, or how to add a new SSH key. Unlike tutorials, how-to guides assume some level of prior knowledge and experience from the user. While tutorials are designed for beginners and cover basic concepts, how-to guides are intended to answer more advanced questions. In the realm of software documentation, how-to guides are typically well-executed and enjoyable to write.  </p> <p>Based on a project experience, how-to guides are sometimes also called <code>runbooks</code>. Each runbook has a specific goal and  provides readers with step-by-step guidance to achieve that goal.  </p> <p>The following list outlines the key points covered in the tutorial documents:    </p> <ul> <li>How-to documentation is problem-oriented.  </li> <li>It focuses on achieving a specific result through a step-by-step guide.  </li> <li>How-to guides are entirely practical.  </li> <li>They are easy to write for technical/product owners.  </li> <li>Readers should have some basic knowledge before starting the guide.  </li> <li>How-to documents can be linked together. Therefore, if one document depends on another, readers can easily navigate between them.  </li> <li>Choose an appropriate title for the guide.  </li> </ul>","tags":["General"]},{"location":"blog/rulebook.html#reference-materials","title":"Reference materials","text":"<p>Reference guides provide technical descriptions of machinery and how to operate it. Unlike how-to guides, reference material focuses solely on information, providing details about functions, fields, attributes, and methods. It may include basic descriptions of how to use the machinery, but it should not attempt to explain basic concepts or provide instructions on how to achieve common tasks. Reference material is straightforward and austere, and may be generated automatically to some extent, but it is never sufficient on its own. For some developers, reference guides are the only kind of documentation they can imagine, assuming that others only need technical information about the software.  </p> <p>We use references every day! API references, Linux commands' references, Python packages'references, and more.</p>","tags":["General"]},{"location":"blog/rulebook.html#discussions","title":"Discussions","text":"<p>Discussions or explanations in documentation clarify and broaden the coverage of a particular topic, providing a higher-level perspective and illuminating the subject matter. Unlike how-to guides or reference material, explanations are understanding-oriented and discursive in nature, often scattered among other sections rather than explicitly created. Discussions can be challenging to create as they require a broader view of the software and a deeper understanding of the subject matter. The division of topics for discussion can sometimes be arbitrary, defined by what the author thinks is a reasonable area to cover at one time rather than a specific task or learning objective.  </p>","tags":["General"]},{"location":"blog/rulebook.html#example","title":"Example","text":"<p>Several websites offer their product documentation using this framework. Let's take a look at one of them: Ubuntu Server documentation!  </p> <p> </p> <p>As we can see, the documentation includes the four main components: Ubuntu Server tutorials: This section of the documentation contains step-by-step tutorials to outline what Ubuntu Server is capable of while helping readers achieve specific aims. The tutorials start with a basic installation guide and continue with a collection of related tutorials and topics that will help readers learn more about Ubuntu Server.  </p> <p>Ubuntu Server how-to guides: The documentation includes a variety of how-to guides that provide readers with specific goals to accomplish. However, it is assumed that readers are already familiar with Ubuntu Server, as the guides may require readers to understand and adapt the steps to fit their specific requirements. While the guides will help readers achieve an end result, they may need to be customized to fit specific needs.  </p> <p>Ubuntu Server explanation guides: This section includes explanatory and conceptual materials aimed at enhancing user comprehension of the functioning and configuration of Ubuntu Server, thereby making it easier to use. The section is divided into three main topics: Software, Network, and Cryptography.    </p> <p>Ubuntu Server reference: This section provides a list of available software and command references. Readers can refer to this documentation to figure out how to interact with different Ubuntu tools and commands.  </p>","tags":["General"]},{"location":"blog/rulebook.html#summary","title":"Summary","text":"<p>Based on the <code>the Grand Unified Theory of Documentation</code> framework, the four distinct components of documentation are interconnected. Tutorials and Discussions are particularly helpful for readers who are just starting to learn, while How-to guides and references are more practical documents that assist in development.  </p> <p> </p> Provided by documentation.divio.com <p>For a software development project, a README file is an essential component, as it serves as a guide to help users understand what the project is about and how to use it. A well-written README file can ensure that a project is more accessible, easier to use, and encourages collaboration among developers. Let's discuss README files in another blog post. ;) </p>","tags":["General"]},{"location":"blog/tags.html","title":"List of all blogs","text":"<p>{{ tag_content }}</p>"},{"location":"blog/tf-mars.html","title":"Moving HashiCorp Terraform state file","text":"<p>This isn't a blog about board games, but HashiCorp Terraform makes me think of the Terraforming Mars board game!</p> <p>Terraforming Mars is a board game designed by Jacob Fryxelius and published by FryxGames in 2016. The game is set in the future, where players take on the role of corporations that work together to terraform Mars and make it habitable for human life. You can check more here: BoardGameGeek</p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#terraform-state-file","title":"Terraform state file","text":"<p>Infrastructure as Code (IaC) enables us to use code to manage infrastructure resources. This approach makes it easier to manage complex infrastructures, reduce manual errors, and increase efficiency.</p> <p>These days HashiCorp Terraform is one of the popular IaC tools. It supports a wide range of Cloud providers and services including AWS, Azure, GCP, K8S, and many others. This enables infrastructure engineers to manage their infrastructure resources in a consistent way, regardless of the cloud provider they are using.</p> <p>Terraform provides a state management mechanism to track the state of the infrastructure resources. This allows us to understand the current state of the infrastructure, identify changes that have been made, and easily make updates. Terraform stores the current state of the infrastructure in a file called tfstate. This state is used by Terraform to map real world resources to the configuration, keep track of metadata, and to improve performance for large infrastructures.</p> <p>This state file is stored locally by default in a file called <code>terraform.tfstate</code>. Terraform utilizes the state file to generate plan and carry out modifications to the infrastructure. Terraform performs a refresh before carrying out any action to update the state with the current state of the infrastructure. That\u2019s why we see <code>Refreshing state\u2026</code> in each Terraform plan output.</p> <pre><code>$ terraform plan\naws_dynamodb_table.dynamodb_locktable: Refreshing state... [id=terraforming-mars-locktable]\naws_s3_bucket.s3_tfstate: Refreshing state... [id=terraforming-mars-tfstate]\n</code></pre> <p>You can read more about Terraform state purpose here: Purpose of Terraform State</p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#terraform-backend","title":"Terraform backend","text":"<p>Terraform enables us to collaborate with other members of our team by using version control systems such as Git. This makes it easier to share infrastructure code, review changes, and ensure that everyone is working on the same version of the infrastructure. However, using a local Terraform state file can be challenging because everyone must make sure to pull the latest tfstate file locally and ensure that nobody else is running Terraform at the same time.</p> <p>To solve this issue, Terraform introduces remote state. Using remote state, the state file can be written to a remote data store. Now, teammates can collaborate on a project without any concern about the latest tfstate file version. Remote state is implemented by a backend or by Terraform Cloud.</p> <p>Terraform supports various types of backends, including AWS S3, Azure Blob Storage, and HashiCorp Consul. These backends provide remote storage for Terraform state files, making it easier to manage infrastructure resources across teams and environments. When using a remote backend, Terraform can read the current state and apply changes to the infrastructure based on that state.</p> <p>However, what if Terraform executed concurrently? Terraform has an ability to lock the state file. Whenever there is a possibility of writing state, the process of state locking occurs automatically. Backends are responsible for providing an API for state locking and state locking is optional.</p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#default-backend","title":"Default backend","text":"<p>Terraform uses a backend called <code>local</code> as the default option, which stores the state data locally as a file on the disk. It means that we do not need to add backend block configuration. For example, the below code block shows the terraform block configured with aws provider:</p> <pre><code>    terraform {\n      required_providers {\n        aws = {\n          source = \"hashicorp/aws\"\n        }\n      }\n    }\n</code></pre> <p>The state file in Terraform is typically stored locally in the current project directory. However, you may wonder how to store the tfstate file in a different location. This can be accomplished by specifying a backend configuration in your Terraform code, which tells Terraform where to store the state file:</p> <pre><code>    terraform {\n      required_providers {\n        aws = {\n          source = \"hashicorp/aws\"\n        }\n      }\n      backend \"local\" {\n        path = \"local_path/terraform.tfstate\"\n      }\n    }\n</code></pre> <p>By adding the backend configuration block and running <code>terraform init</code>, you will get an error message indicates change in backend configuration:</p> <pre><code>    $ terraform init\n    Initializing the backend...\n    Error: Backend configuration changed\n\n    A change in the backend configuration has been detected, which may require migrating existing state.\n\n    If you wish to attempt automatic migration of the state, use \"terraform init -migrate-state\".\n    If you wish to store the current configuration with no changes to the state, use \"terraform init -reconfigure\".\n</code></pre> <p>The error message simply explains the root cause and the possible solutions. The <code>-migrate-state</code> option will attempt to copy existing state to the new backend, and depending on what changed, may result in interactive prompts to confirm migration of workspace states. On the other hand, the <code>-reconfigure</code> option disregards any existing configuration, preventing migration of any existing state.  </p> <p>If you are trying to move the state file from the default working directory to your custom directory, <code>-migrate-state</code> is the correct option.</p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#migrating-to-a-remote-backend","title":"Migrating to a remote backend","text":"<p> Now, how can we move the local state file of a current project to a remote backend? As we understood, using a remote backend can help improve collaboration, scalability, security, and ease of management when working with Terraform.</p> <p>I would like to divide the supported backends into two categories: Local and Remote. In the Local group, the state file is stored locally (default or using a local configuration). The Remote group includes options such as Terraform Cloud, AWS S3, Azurerm, and others.</p> <p>HashiCorp says that remote backend is unique among all other Terraform backends. Read more about it here: Terraform Remote Backend</p> <p>In this demonstration, I try to use AWS S3 backend. AWS S3 backend supports state locking via AWS DynamoDB. It means that it doesn\u2019t support state locking out of the box.</p> <p>As an example of an in-the-box locking feature, Azurerm supports state locking and consistency checking with Azure Blob Storage native capabilities.</p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#implementation","title":"Implementation","text":"<p>In the first step, let's create resources an AWS to support storing Terraform project state file and status. Based on a project experience, I have a project called <code>iac-base</code> includes all the base infrastructure for other projects deployment. The below code block shows <code>iac-base</code> resources:  </p> <pre><code>    # S3 bucket \n    resource \"aws_s3_bucket\" \"s3_tfstate\" {\n      bucket = \"terraforming-mars-tfstate\"\n    }\n\n    # S3 bucket ACL\n    resource \"aws_s3_bucket_acl\" \"s3_acl\" {\n      bucket = aws_s3_bucket.s3_tfstate.id\n      acl    = \"private\"\n    }\n\n    # S3 bucket encryption\n    resource \"aws_s3_bucket_server_side_encryption_configuration\" \"s3_encryption\" {\n      bucket = aws_s3_bucket.s3_tfstate.bucket\n\n      rule {\n        apply_server_side_encryption_by_default {\n          sse_algorithm = \"aws:kms\"\n        }\n      }\n    }\n\n    resource \"aws_s3_bucket_versioning\" \"s3_bucket_versioning\" {\n      bucket = aws_s3_bucket.s3_tfstate.id\n      versioning_configuration {\n        status = \"Enabled\"\n      }\n    }\n\n    resource \"aws_s3_bucket_lifecycle_configuration\" \"s3_bucket_retention_policy\" {\n      bucket     = aws_s3_bucket.s3_tfstate.id\n      depends_on = [aws_s3_bucket_versioning.s3_bucket_versioning]\n\n\n      rule {\n        status = \"Enabled\"\n        id     = \"retention_policy\"\n        noncurrent_version_expiration {\n          noncurrent_days = 180\n        }\n\n      }\n    }\n\n    resource \"aws_s3_bucket_public_access_block\" \"bucket_block_public\" {\n      bucket = aws_s3_bucket.s3_tfstate.id\n\n      block_public_acls       = true\n      block_public_policy     = true\n      ignore_public_acls      = true\n      restrict_public_buckets = true\n    }\n\n    # DynamoDB \n    resource \"aws_dynamodb_table\" \"dynamodb_locktable\" {\n      name           = \"terraforming-mars-locktable\"\n      hash_key       = \"LockID\"\n      billing_mode   = \"PROVISIONED\"\n      write_capacity = 1\n      read_capacity  = 1\n\n      attribute {\n        name = \"LockID\"\n        type = \"S\"\n      }\n    }\n</code></pre> <p>The above code block creats a AWS S3 bucket based on the best practices and a DynamoDB table for state locking. After applying the configuration, your base resources to store projects state files is ready. Now we are ready to migrate projects state file from local to AWS S3 remote backend. Modify your Terraform code block to add AWS remote backend configuration:</p> <pre><code>    terraform {\n      required_providers {\n        aws = {\n          source = \"hashicorp/aws\"\n        }\n      }\n      backend \"s3\" {\n        bucket         = \"terraforming-mars-tfstate\"\n        key            = \"terraform.state\"\n        region         = \"eu-west-1\"\n        encrypt        = true\n        dynamodb_table = \"terraforming-mars-locktable\"\n      }\n    }\n</code></pre> <p>I created base resources in <code>eu-west-1</code> region. You should use the correct region based on your configuration.</p> <p>I also migrate <code>iac-base</code> Terraform state file to this remote backend.</p> <p>Migration from local to a remote backend is EASIER than moving resources from Earth to Mars. You only need to run <code>terraform init</code>. Terraform detects the new backend configuration, and asks about migrating:  </p> <pre><code>    $ terraform init \n\n    Initializing the backend...\n    Do you want to copy existing state to the new backend?\n      Pre-existing state was found while migrating the previous \"local\" backend to the\n      newly configured \"s3\" backend. No existing state was found in the newly\n      configured \"s3\" backend. Do you want to copy this state to the new \"s3\"\n      backend? Enter \"yes\" to copy and \"no\" to start with an empty state.\n\n      Enter a value: yes\n\n    Releasing state lock. This may take a few moments...\n\n    Successfully configured the backend \"s3\"! Terraform will automatically\n    use this backend unless the backend configuration changes.\n\n    Initializing provider plugins...\n    - Reusing previous version of hashicorp/aws from the dependency lock file\n    - Using previously-installed hashicorp/aws v4.57.0\n\n    Terraform has been successfully initialized!\n\n    You may now begin working with Terraform. Try running \"terraform plan\" to see\n    any changes that are required for your infrastructure. All Terraform commands\n    should now work.\n\n    If you ever set or change modules or backend configuration for Terraform,\n    rerun this command to reinitialize your working directory. If you forget, other\n    commands will detect it and remind you to do so if necessary.\n</code></pre> <p>One of the core concepts of the IaC is about writing one time, and use several times. For example, you can use the same resource implementation to deploy in several environments such as Development, Test, Stage, or Production. Then it comes to a concept called multi-account and multi-backend architecture. I will discuss this concept in a separate blog. </p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#changing-the-s3-bucket","title":"Changing the S3 Bucket","text":"<p>I always say it is better to consider all details before implementation. A design document including all the project details can prevent most future issues. For instance, naming conventions is one of my criteria. But it might happen that you should change the state S3 bucket to another bucket. In this case, Terraform can move your state file from one bucket to another bucket using <code>terraform init -migrate-state</code>. In the below code block, I try to move state file from <code>terraforming-mars-tfstate</code> bucket to <code>terraforming-venus-next-tfstate</code>:  </p> <pre><code>    terraform {\n      required_providers {\n        aws = {\n          source = \"hashicorp/aws\"\n        }\n      }\n      backend \"s3\" {\n        bucket         = \"terraforming-venus-next-tfstate\"\n        key            = \"terraform.state\"\n        region         = \"eu-west-1\"\n        encrypt        = true\n        dynamodb_table = \"terraforming-venus-next-locktable\"\n      }\n    }\n</code></pre>","tags":["Terraform"]},{"location":"blog/tf-mars.html#best-practices","title":"Best practices","text":"<p>Enable encryption for S3 bucket: Using encryption for the state file in the S3 bucket. State files can contain secrets, keys, etc. in plaintext. So, it is important to keep it encrypted. AWS S3 backend supports different encryption methods: </p> <ul> <li><code>encrypt</code> - Enable server side encryption of the state file.  </li> <li><code>kms_key_id</code> - Amazon Resource Name (ARN) of a Key Management Service (KMS) Key to use for encrypting the state. Note that if this value is specified, Terraform will need kms:Encrypt, kms:Decrypt and kms:GenerateDataKey permissions on this KMS key.  </li> <li><code>sse_customer_key</code> - The key to use for encrypting state with Server-Side Encryption with Customer-Provided Keys (SSE-C). This is the base64-encoded value of the key, which must decode to 256 bits. This can also be sourced from the <code>AWS_SSE_CUSTOMER_KEY</code> environment variable, which is recommended due to the sensitivity of the value. Setting it inside a terraform file will cause it to be persisted to disk in terraform.tfstate.  </li> </ul> <p>Enable S3 bucket versioning: Enabling bucket versioning on the S3 bucket is strongly advised as it enables recovery of the state in case of unintended deletions and mistakes.  </p> <p>Enable retention lifecycle policy: As S3 bucket versioning enables, it is wise to have a retention lifecycle policy to delete the old state file objects. You can add <code>noncurrent_version_expiration</code> policy based on your project/organization definition.  </p> <p>Suggested structure for single-environment projects: This is only a suggestion based on my experience with how to have a structure for your Terraform projects. As we discussed it is a good idea to have a project called <code>iac-based</code> including all your base configurations. For example, resources for your Terraform backend:</p> <pre><code>$ tree                                                                               \n.\n\u251c\u2500\u2500 iac-base\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 search-planet-x-state.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tf-mars-state.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tf-venus-state.tf\n\u251c\u2500\u2500 searching-for-planet-x\n\u251c\u2500\u2500 terraforming-mars\n\u2514\u2500\u2500 terraforming-venus\n</code></pre>","tags":["Terraform"]},{"location":"trainings/docker/docker-file.html","title":"Dockerfile","text":""},{"location":"trainings/docker/docker-file.html#intro","title":"Intro","text":"<p>In the previous section, we learned how to create a new image from a running container. In this section, we will explore the process of creating a custom Docker image using a manifest called Dockerfile.  </p> <p>Let's begin with an example. Imagine you have developed a Python code and now you wish to run it on a new server. Essentially, you need to install an operating system and all the necessary packages to execute your code. The diagram below illustrates the steps involved:</p> <p></p> <p>In a Dockerfile, we can refer to the operating system mentioned in the above diagram as the base image. In a Dockerfile, the base image refers to the starting point for building a new Docker image. It is the foundation upon which your custom image is built. The base image contains the operating system and other dependencies required for your application to run.  </p> <p>After specifying the base image, we can proceed to run commands in the Dockerfile to install Python and dependency packages, as well as copy files into the image. Finally, we can create an entry point that will be responsible for running the code.</p> <p></p> <p>Dockerfile is a text based flow of the above diagram. A Dockerfile is a text file that contains a set of instructions and commands used to build a Docker image. It serves as a blueprint or recipe for creating a customized Docker image. The Dockerfile specifies the base image to start from, as well as the steps needed to install dependencies, configure the environment, and copy files into the image.</p>"},{"location":"trainings/docker/docker-file.html#dockerfile-structure","title":"Dockerfile structure","text":"<p>Let's revisit our Python code example and align the execution flow with the structure of a Dockerfile.</p> <p></p> <p>The structure of a Dockerfile typically follows a specific format and order of instructions. Here is a general outline of a Dockerfile structure:</p> <ol> <li> <p>Base Image: The first instruction in a Dockerfile is usually the <code>FROM</code> instruction, which specifies the base image to use. It defines the starting point for building your custom image. For example: <code>FROM ubuntu:latest</code></p> </li> <li> <p>Environment Configuration: This section includes instructions to configure the environment within the Docker image. It often involves installing dependencies, setting environment variables, and performing system-level configurations. Common instructions used here include <code>RUN</code>, <code>ENV</code>, and <code>ARG</code>.</p> </li> <li> <p>File Copy: In this section, you can copy files and directories from the host machine into the Docker image using the <code>COPY</code> or <code>ADD</code> instructions. This allows you to include application code, configuration files, and other necessary assets in the image.</p> </li> <li> <p>Working Directory: The <code>WORKDIR</code> instruction sets the working directory for subsequent instructions. It is a good practice to set a specific directory where the application code and files will reside within the image.</p> </li> <li> <p>Execution Commands: This section includes instructions to run commands or scripts inside the Docker image during the build process. The <code>RUN</code> instruction is commonly used here to install packages, compile code, or perform other necessary tasks.</p> </li> <li> <p>Expose Ports: The <code>EXPOSE</code> instruction specifies the network ports on which the container will listen at runtime. It is used to document which ports should be published when running the container.</p> </li> <li> <p>Container Execution: The final instructions in a Dockerfile define the command or entry point that will be executed when the container is run. This can be specified using the <code>CMD</code> or <code>ENTRYPOINT</code> instruction.</p> </li> </ol> <p>It's important to note that the structure and contents of a Dockerfile can vary depending on the specific requirements of your application or project. However, the outlined structure provides a general guideline for creating Dockerfiles.</p> <p>You should understand how to use the syntax in a Dockerfile, but there's no need to memorize all the Dockerfile syntax. You can refer to the Dockerfile reference documentation to find detailed information about each instruction and its usage. This documentation provides comprehensive details on all the available instructions and their options. It serves as a valuable resource to assist you in creating and understanding Dockerfile syntax.</p>"},{"location":"trainings/docker/docker-file.html#the-first-dockerfile","title":"The first Dockerfile","text":"<p>In this section, we will be writing our first Dockerfile and creating a custom image to run a Python Flask application.</p> <p>You can find all manifest and sample codes in here.</p> <p>Referring to the previous diagram, we require a manifest to set up an operating system known as the base image. Additionally, we need to install Python along with its dependencies, including pip. Finally, we'll copy the Python source code into the image and define the command for execution. The below code lines is a manifest describing all these steps:  </p> <pre><code># Use Ubuntu as the base image\nFROM ubuntu:latest\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Update packages and install necessary dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    python3 \\\n    python3-pip\n\n# Install Python dependencies\nRUN pip3 install flask\n\n# Copy the application code into the container\nCOPY . .\n\n# Expose the port on which the Flask app will listen\nEXPOSE 5000\n\n# Define the command to run the Flask app\nCMD [\"python3\", \"app.py\"]\n\n</code></pre> <p>This manifest, commonly referred to as the Dockerfile, should be saved in a file named <code>Dockerfile</code>. To build the image based on the Dockerfile, you can use the <code>docker build</code> command.</p> <pre><code>docker build -t first .\n</code></pre> <p><code>-t &lt;image_name&gt;</code> specifies the name and optionally a tag for the image. <code>&lt;path_to_Dockerfile&gt;</code> is the path to the directory containing the Dockerfile. You can use <code>.</code> to indicate the current directory if the Dockerfile is located there.</p> <p>Congratulations! You have successfully built your first custom image, which is now ready to be used. To run a container using the newly created image, you can follow these steps:</p> <pre><code>docker run first\n</code></pre> <p>By executing the <code>docker run</code> command, a container will be created and started based on the specified image. You can now access and interact with your Python Flask application inside the running container. You can access to the app with http://localhost:5000 .</p> <p>Do we always require an OS base image like Ubuntu to build our custom image? Are there alternative solutions for creating an image for your Python code?</p> <p>While using an OS base image like Ubuntu is a common approach, it is not the only solution for building Docker images for Python code. Docker provides specialized base images specifically designed for running Python applications. These images, such as <code>python</code>, <code>python-alpine</code>, or <code>python-slim</code>, come with the necessary dependencies and configurations for Python development.</p> <p>These Python-specific base images are generally smaller in size compared to full OS base images like Ubuntu. They are optimized for running Python applications and provide a more lightweight option. Using a Python-specific base image can help reduce the overall image size and improve the efficiency of your Docker containers.</p> <p>However, the choice of base image depends on your specific requirements. If your application requires specific OS-level dependencies or configurations, an OS base image like Ubuntu may be more suitable. It allows you to have greater control and flexibility over the environment.</p> <p>Ultimately, it is essential to consider your application's needs and choose the appropriate base image accordingly. Let's write the second Dockerfile and improve the first one by using a Python base image instead of Ubuntu.</p> <p>Here's an example of a revised Dockerfile using a Python base image:</p> <pre><code># Use Python as the base image\nFROM python:3.9-slim\n\n# Set the working directory inside the container\nWORKDIR /app\n\n# Install Python dependencies\nRUN pip3 install flask\n\n# Copy the application code into the container\nCOPY . .\n\n# Expose the port on which the Flask app will listen\nEXPOSE 5000\n\n# Define the command to run the Flask app\nCMD [\"python3\", \"app.py\"]\n</code></pre>"},{"location":"trainings/docker/docker-image.html","title":"Docker Image","text":""},{"location":"trainings/docker/docker-image.html#intro","title":"Intro","text":"<p>A Docker image is a lightweight, standalone, and executable software package that contains everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and dependencies. It is a portable and self-sufficient unit that encapsulates an application or service.</p> <p>Images in Docker follow a layered architecture. This layered approach allows for efficient storage and sharing of images. When a Docker image is built, only the layers that have changed since the previous build need to be rebuilt, saving time and resources. We will see more details about this layer structure and then we can create a manifest of each layers to build the image. This manifest is called <code>Dockefile</code>.</p> <p>Docker images are stored in a Docker registry, such as Docker Hub or a private registry. They can be easily shared, distributed, and pulled onto different environments, allowing for consistent deployment across different systems. When a Docker image is run, it creates a Docker container, which is an isolated and lightweight runtime instance of the image.</p> <p>Using Docker images provides benefits such as portability, reproducibility, scalability, and consistency across different environments, making it easier to develop, deploy, and manage applications and services.</p> <p>In this section, we will explore all these concepts and start working with Docker image cli.</p>"},{"location":"trainings/docker/docker-image.html#docker-image-on-surface","title":"Docker image on surface","text":"<p>During previous sessions, we learned how to run a container and connect to it. We also saw how to execute commands and make changes within the running container. </p> <p>In this section, we will delve into creating a new image from a running container. We will explore the finer details of Docker images and ensure that we are well-prepared to create a custom image using the <code>Dockefile</code> manifest.</p> <p>As a high-level definition, a Docker image is a lightweight, standalone, and executable software package that contains everything needed to run a piece of software, including the code, runtime, system tools, system libraries, and dependencies. It is a portable and self-sufficient unit that encapsulates an application or service.</p>"},{"location":"trainings/docker/docker-image.html#docker-registry","title":"Docker registry","text":"<p>A Docker registry is a central storage and distribution system for Docker images. It serves as a repository where Docker images can be stored, versioned, and shared among different users and systems.</p> <p>A remote Docker registry is a registry that is hosted on a remote server or cloud platform. Examples of remote Docker registries include Docker Hub (the default public registry), Amazon Elastic Container Registry (ECR), Google Container Registry, and others. Remote registries provide a convenient way to access and share Docker images globally. They allow users to pull and push images to/from a central repository that is accessible over the network. Docker engine by default tries to download images from Docker hub.</p> <p>On the other hand, a local Docker registry is a registry that is hosted on your local machine or on a local network. It is typically used for private or internal image distribution within an organization. Local registries provide a way to store and manage Docker images within your local infrastructure, ensuring better control and security over the images. </p> <p>We will see later how to create our own Docker registry.</p>"},{"location":"trainings/docker/docker-image.html#pulling-image","title":"Pulling image","text":"<p>The <code>docker pull</code> command is used to download Docker images from a container registry or repository. Here's how it works:</p> <ol> <li> <p>When you run the <code>docker pull</code> command, you specify the name of the image you want to download, along with any optional tags or versions. For example: <code>docker pull nginx</code> or <code>docker pull ubuntu:latest</code>.</p> </li> <li> <p>Docker searches for the specified image in the local Docker engine's cache. If the image is found locally and matches the requested version or tag, Docker uses that image directly. If the image is not found locally or if you specifically want to download a fresh copy, Docker proceeds to the next step.</p> </li> <li> <p>Docker contacts the container registry specified in the image name (e.g., Docker Hub, a private registry, or another publicly available registry) to check if the requested image exists there.</p> </li> <li> <p>If the image is found in the registry, Docker starts downloading the image layers. Docker images are composed of multiple layers, which are incremental changes on top of each other. These layers are downloaded and assembled to create the complete image on your local system.</p> </li> </ol> <p>Once the image is fully downloaded and assembled, it is stored in the local Docker engine's cache for future use. Docker can improve performance and efficiency when working with images by local storage mechanism called Docker image cache. When you download images (<code>pull</code>) or build images, the downloaded image layers are stored in the image cache on your local machine.</p> <p>The image cache serves two main purposes:</p> <ul> <li> <p>Faster image retrieval: Once an image layer is downloaded or built, it is stored in the cache. If you need to use the same image again in the future, Docker can retrieve the layers from the cache instead of downloading or rebuilding them. This greatly speeds up the process of creating new containers based on already downloaded images.</p> </li> <li> <p>Layer reuse: Docker image layers are reusable across different images. If multiple images share the same base layers, those layers are stored in the cache and can be reused across different images. This saves disk space as well as download or build time, as the common layers don't need to be duplicated for each image.</p> </li> </ul> <p>The image cache is managed by the Docker engine and is located in a specific directory on your local machine. By default, Docker keeps the cache at <code>/var/lib/docker</code> on Linux systems. </p> <p>You can check list of images by running below commands:</p> <pre><code>docker images \nOR\ndocker image ls\n</code></pre> <p>The cache can grow in size as you pull or build more images, so it's important to periodically clean the cache to reclaim disk space if necessary. You can delete unused images using the below command:  </p> <pre><code>docker rmi &lt;image-id&gt;\n</code></pre> <p>As an image may consist of multiple layers, you can remove all unused cache layers by following these steps:</p> <pre><code>docker image prune -a\n</code></pre>"},{"location":"trainings/docker/docker-image.html#tacking-snapshot","title":"Tacking snapshot","text":"<p>Considering that you have run a container with custom configurations, you may now want to create a new container with the same configuration. The <code>docker commit</code> command in Docker is used to create a new image from changes made to a running container. It allows you to capture the current state of a container and save it as a new image that can be used to create new containers in the future.</p> <p>The basic syntax of the <code>docker commit</code> command is as follows:</p> <pre><code>docker commit [OPTIONS] CONTAINER [REPOSITORY[:TAG]]\n</code></pre> <p>Now, you can use the new created image to run your custom container. But what if you need to run the container in a different environment or if a colleague asks you to run the same container?</p> <p>The <code>docker save</code> command in Docker is used to save one or more Docker images as a tar archive. This command allows you to export Docker images from your local system so that they can be stored or transferred to another machine or shared with others. You can simply store the image as a tar file using the below command:</p> <pre><code>docker save -o image.tar.gz &lt;image_id&gt;\n</code></pre> <p>Now, to facilitate sharing, you can share the image tar file. In the other environment, you can easily load the image into Docker by using the following command:</p> <pre><code>docker load -i &lt;path/to/image.tar.gz&gt;\n</code></pre> <p>In Docker, the <code>docker import</code> and <code>docker export</code> commands are used to import and export containers as file system snapshots, respectively. Read more about docker import and docker export!</p>"},{"location":"trainings/docker/docker-image.html#docker-image-deep-dive","title":"Docker image deep dive","text":"<p>In the Docker volume section, we introduced Union File System (UnuinFS). Infact, UnionFS is used as the underlying file system mechanism for Docker images. </p> <p>Union File System (UnionFS) is a Linux kernel technology that allows for the merging of multiple file systems into a single unified view. It enables the creation of layered file systems by stacking multiple file systems on top of each other. Each layer appears as a separate directory, but the files and directories from lower layers are also accessible.</p> <p>Here are some key concepts related to UnionFS:</p> <ul> <li> <p>Layers: UnionFS operates using a layered approach. Each layer represents a separate file system. When accessing a file, UnionFS scans through the layers from top to bottom until it finds the file or reaches the bottom-most layer. This layering allows for file systems to be combined while preserving the original content.</p> </li> <li> <p>Copy-on-Write (CoW): UnionFS employs a copy-on-write mechanism to handle modifications to files. When a file is modified in an upper layer, UnionFS creates a new copy of the file in that layer without modifying the original file in the lower layer. This ensures that changes are isolated and don't affect lower layers. It also optimizes storage by only storing modified or new files in the upper layers.</p> </li> <li> <p>Read-only Base Layer: The bottom-most layer in a UnionFS stack is typically a read-only base layer that contains the original file system or image. It remains unchanged, while new layers are added on top for modifications or additions.</p> </li> <li> <p>OverlayFS: OverlayFS is a specific implementation of UnionFS in the Linux kernel. It provides a union mount that allows the merging of multiple directories into a single virtual directory. OverlayFS is the most commonly used UnionFS implementation in recent versions of Linux and is utilized by Docker for managing volumes.</p> </li> </ul> <p>Let's simulate a Docker image using OverlayFS involves creating a layered file system manually. </p> <p></p> <p>Create three directories: <code>/lower</code> for the lower layer (base directory), <code>/upper</code> for the upper layer (container directory), and <code>/merged</code> for the merged view.</p> <pre><code>sudo mkdir /lower\nsudo mkdir /upper\nsudo mkdir /merged\n</code></pre> <p>Create a text file in <code>/lower</code> and put a sample text: <code>Here is the lower layer</code>. Then create a another file in <code>/upper</code> and put a sample text: <code>Here is the upper layer</code>.</p> <p>Mount the OverlayFS with the lower and upper directories:</p> <pre><code>sudo mount -t overlay overlay -o lowerdir=/lower,upperdir=/upper,workdir=/merged /merged\n</code></pre> <p>This command mounts the OverlayFS and uses /lower as the lower directory, /upper as the upper directory, and /merged as the merged view.</p> <p>Navigate to the <code>/merged</code> and check the result. </p> <p>This is a simplified simulation and does not cover all the functionalities and features of Docker images. Docker images include various metadata, layer management, and compression mechanisms, which are not replicated in this manual simulation. This simulation merely demonstrates the basic concept of layering file systems using UnionFS, similar to how Docker images are structured.</p>"},{"location":"trainings/docker/docker-image.html#practice-your-knowledge","title":"Practice your knowledge","text":"<p>You can practice these tasks in a terminal or command prompt with Docker installed on your machine. Make sure you have Docker properly installed and configured before starting the practice scenario.</p> <p>Remember to refer to Docker's official documentation for detailed information on each command and its usage. Happy learning!  </p>"},{"location":"trainings/docker/docker-image.html#scenario","title":"Scenario","text":"<p>Task 1: Run an Ubuntu container </p> <p>Run a container with the the latest version of Ubuntu image. Export the below environment variables:  </p> <pre><code>VAR1=ubuntu\nVAR2=webserver\n</code></pre> <p>Map port <code>3000</code> of host to the container, and finally call the container <code>ubuntu001</code>.</p> <p>Task 2: Customize the container </p> <p>Try to intall the below packages:</p> <ul> <li>vim</li> <li>Python</li> <li>Nginx</li> </ul> <p>Create a custom Nginx configuration to listen on port 3000, and show a custom welcome screen (Trying to copy <code>index.html</code> from host to the container).</p> <p>Task 3: Docker commit</p> <p>se the <code>docker commit</code> command to create a new image from the modified container, and verify the image is available.</p> <p>Task 4: Docker save</p> <p>Next, use the <code>docker save</code> command to export the newly created image as a tar file, and share it with another colleague. </p> <p>Task 5: Run a new continer</p> <p>Load the new image and run a new container. Navigate to the <code>http://localhost:3000</code> to verify the new container. </p>"},{"location":"trainings/docker/docker-network.html","title":"Docker Networking","text":""},{"location":"trainings/docker/docker-network.html#intro","title":"Intro","text":"<p>Docker networking refers to the system that allows communication between Docker containers, as well as between containers and the host machine or external networks. Docker provides various networking options that enable containers to connect and communicate with each other, forming distributed applications or microservices.</p>"},{"location":"trainings/docker/docker-network.html#network-types","title":"Network types","text":"<p>Docker supports different types of networks, including bridge networks, host networks, overlay networks, and macvlan networks.</p> <ul> <li> <p>Bridge networks: This is the default networking mode in Docker. Containers connected to the same bridge network can communicate with each other, and the host machine can also communicate with containers on the bridge network.</p> </li> <li> <p>Host networks: In this mode, containers share the host machine's network stack directly. They use the host's IP address, and there is no isolation between the host and containers in terms of networking.</p> </li> <li> <p>Overlay networks: Overlay networks enable communication between containers across multiple Docker hosts, forming a swarm cluster. This allows you to create distributed applications that span multiple machines.</p> </li> <li> <p>Macvlan networks: Macvlan networks allow containers to have their own MAC addresses and appear as separate devices on the physical network. This can be useful in scenarios where you need containers to have direct access to the physical network.</p> </li> </ul>"},{"location":"trainings/docker/docker-network.html#inspect-networks","title":"Inspect networks","text":"<p>To check the existing Docker networks on your system, you can use the Docker CLI and run the following command:  </p> <pre><code>docker network ls\n</code></pre> <p>This command lists all the available Docker networks along with their details, such as the network ID, name, driver, and scope. For example:  </p> <pre><code>NETWORK ID     NAME            DRIVER    SCOPE\ncb86147a15bd   bridge          bridge    local\n5dab3e75f42d   host            host      local\ne8f44c5252de   none            null      local\n</code></pre> <p>In this example, you can see multiple networks listed, including the default bridge network (<code>bridge</code>), and host network (<code>host</code>). </p> <p>In Docker, the <code>null</code> network type is a special type of network that provides a container with no networking capabilities. When a container is connected to the <code>null</code> network, it effectively isolates the container from all forms of network communication, including communication with other containers, the host machine, and external networks. We will see an example during the practice scenario.</p> <p>To get more detailed information about a specific network, you can use the <code>docker network inspect</code> command followed by the network name or ID. For example:</p> <pre><code>docker network inspect &lt;network&gt;\n</code></pre> <p>This command will display the detailed configuration and settings of the specefic network. By using these commands, you can easily check the available Docker networks and their associated properties on your system.</p> <p>You can apply inspect command on each container to find container network datails:  </p> <pre><code>docker inspect &lt;container_id_or_name&gt;\n</code></pre> <p>This command will provide detailed information about the container, including its network configuration. The output will include a section called <code>NetworkSettings</code> that contains the network-related details, such as the container's IP address, connected networks, exposed ports, and more:</p> <pre><code>[\n    {\n        ...\n        \"NetworkSettings\": {\n            \"Networks\": {\n                \"bridge\": {\n                    \"IPAMConfig\": null,\n                    \"Links\": null,\n                    \"Aliases\": [\n                        \"container_name\"\n                    ],\n                    \"NetworkID\": \"network_id\",\n                    \"EndpointID\": \"endpoint_id\",\n                    \"Gateway\": \"gateway_ip\",\n                    \"IPAddress\": \"container_ip\",\n                    \"IPPrefixLen\": 24,\n                    ...\n                }\n            }\n        }\n        ...\n    }\n]\n\n</code></pre>"},{"location":"trainings/docker/docker-network.html#custom-network","title":"Custom network","text":"<p>Creating custom networks in Docker provides flexibility, better organization, and control over network communication within your containerized applications. It allows for easier management, improved security, and scalability, making it a valuable tool for various use cases.</p> <p>You can create a custom network using the belwo command: </p> <pre><code>docker network create &lt;network_name&gt;\n</code></pre> <p>The command creates a fresh network using the specified DRIVER , which can be either the built-in <code>bridge</code> or <code>overlay</code> network drivers. If you have a custom network driver installed, you can also specify it here. If no <code>--driver</code> option is provided, the command defaults to creating a bridge network (The above command). Upon Docker Engine installation, a bridge network is automatically created, corresponding to the <code>docker0</code> bridge that Engine traditionally relies on. When launching a container with docker run, it automatically connects to this default bridge network. Although the default bridge network cannot be removed, you can create additional networks using the network create command.</p> <p>Then you can assign the network to any conatiner using <code>--network</code> switch: </p> <pre><code>docker run --network=&lt;network_name&gt; &lt;image&gt;\n</code></pre> <p>If you want to add a container to a network after the container is already running, use the <code>docker network connect</code> subcommand.</p> <p>You can connect multiple containers to the same network. Once connected, the containers can communicate using only another container\u2019s IP address or name.</p> <p>You can disconnect a container from a network using the <code>docker network disconnect</code> command.</p> <p>You can read more details about docker networking in Docker Networking Overview and find docker network create deatils here: docker network create.</p>"},{"location":"trainings/docker/docker-network.html#host-type","title":"Host type","text":"<p>The \"host\" network mode in Docker is primarily used in specific scenarios where you need maximum performance and want containers to share the network stack of the host machine. Here are a few common use cases for the \"host\" network mode:</p> <p>Performance Optimization: In some cases, you may have applications that require low latency and high network throughput. By using the \"host\" network mode, containers can directly leverage the host machine's network stack, eliminating any network virtualization overhead and achieving maximum performance.</p> <p>Host Networking Dependencies: Certain applications or services may have dependencies on specific network configurations or services running on the host machine. By using the \"host\" network mode, containers can directly access these host services without any additional network configuration. This can simplify the deployment and reduce potential networking issues.</p> <p>Network Troubleshooting and Monitoring: When troubleshooting or monitoring network-related issues, the \"host\" network mode can be beneficial. Containers in the \"host\" mode have visibility into the entire host network stack, allowing you to analyze network traffic, monitor network performance, or troubleshoot connectivity problems effectively.</p> <p>Limited Network Isolation Requirements: In some cases, network isolation may not be a requirement. For example, if you are running a single container on a host and network isolation is not necessary, using the \"host\" network mode can simplify the setup and eliminate the need for network configuration within the container.</p> <p>It's important to note that using the \"host\" network mode removes network isolation between containers and the host machine. Containers in the \"host\" mode share the same network namespace as the host, meaning they use the same IP address, network interfaces, and ports. This can potentially expose services within the containers to the host network, which may have security implications. Therefore, it's crucial to carefully consider the security requirements and potential risks before using the \"host\" network mode.</p>"},{"location":"trainings/docker/docker-network.html#exposing-ports","title":"Exposing ports","text":"<p>Docker port mapping and exposing are mechanisms that allow you to make specific ports within a container accessible to the host machine or external networks. They enable communication with services running inside the container from outside sources.</p> <ul> <li>Port Mapping (Publishing): Port mapping, also known as port publishing, allows you to map a port exposed by a container to a port on the host machine or external network. It establishes a communication pathway between the host and the container. When running a container, you can use the <code>-p</code> or <code>--publish</code> option with the docker run command to specify the port mapping. For example:  </li> </ul> <pre><code>docker run -p &lt;host_port&gt;:&lt;container_port&gt; &lt;image_name&gt;\n</code></pre> <p>Here, <code>&lt;host_port&gt;</code> represents the port on the host machine, and <code>&lt;container_port&gt;</code> represents the port exposed by the container. By specifying the port mapping, incoming connections to the host port will be forwarded to the corresponding container port.</p> <ul> <li>Port Exposing (Documenting): Port exposing, on the other hand, is used to document the ports that the container listens on. It is achieved using the <code>EXPOSE</code> instruction in the Dockerfile.</li> </ul> <p>The <code>EXPOSE</code> instruction informs Docker that the container listens on specific ports. It serves as documentation for other users, administrators, or developers to know which ports are intended to be used when interacting with the container.</p> <p>For example, in a Dockerfile:</p> <pre><code>...\nEXPOSE 8080\nEXPOSE 9000/tcp\n...\n</code></pre> <p>Here, the Dockerfile states that the container listens on ports 8080 and 9000.</p> <p>We will read more about Dockerfile in the next session.</p>"},{"location":"trainings/docker/docker-network.html#dns-service","title":"DNS Service","text":"<p>Different networks provide different communication patterns (for example by IP address only, or by container name) between containers depending on network type and whether it\u2019s a Docker default or a user-defined network.</p> <p>By default, containers inherit the DNS settings of the host, as defined in the <code>/etc/resolv.conf</code> configuration file. Containers that attach to the default bridge network receive a copy of this file. Containers that attach to a custom network use Docker\u2019s embedded DNS server. The embedded DNS server forwards external DNS lookups to the DNS servers configured on the host.</p> <p>You can configure DNS resolution on a per-container basis, using flags for the <code>docker run</code> or <code>docker create</code> command used to start the container. For example, the below command create a container with a custom DNS and hostname:  </p> <pre><code>docker run --dns &lt;custom_dns&gt; --hostname &lt;custom_hostname&gt; &lt;image_name&gt;\n</code></pre> <p><code>--hostname</code> is the hostname of the container. Defaults to the container\u2019s ID if not specified. Custom hosts, defined in <code>/etc/hosts</code> on the host machine, aren\u2019t inherited by containers. </p>"},{"location":"trainings/docker/docker-network.html#practice-your-knowledge","title":"Practice your knowledge","text":"<p>You can practice these tasks in a terminal or command prompt with Docker installed on your machine. Make sure you have Docker properly installed and configured before starting the practice scenario.</p> <p>Remember to refer to Docker's official documentation for detailed information on each command and its usage. Happy learning!  </p>"},{"location":"trainings/docker/docker-network.html#scenario","title":"Scenario","text":"<p>Implement the diagram below in your Docker lab:</p> <p></p>"},{"location":"trainings/docker/docker-volume.html","title":"Docker Volume","text":""},{"location":"trainings/docker/docker-volume.html#intro","title":"Intro","text":"<p>In Docker, a volume is a mechanism for persistently storing and managing data generated by and used by Docker containers. It provides a way to share data between containers, as well as between the host machine and containers.  </p> <p>A Docker volume is a directory that is managed by Docker and is isolated from the container's file system. Volumes are typically created and managed using Docker commands or through Docker Compose files.  </p> <p>When a volume is created, it is associated with a specific container or multiple containers. The data stored in a volume persists even if the associated container is stopped or deleted. This allows for data to be shared and reused across different container instances.  </p> <p>Volumes have several advantages over other mechanisms, such as bind mounts. They are not dependent on the directory structure or operating system of the host machine, making them more portable. Volumes also support features like volume drivers, which enable integration with external storage systems, and can be easily managed and backed up by Docker.  </p> <p>Overall, volumes provide a flexible and efficient way to manage data in Docker containers, allowing for data persistence and sharing across containers and deployments.  </p>"},{"location":"trainings/docker/docker-volume.html#unionfs","title":"UnionFS","text":"<p>Docker volumes are created and managed using a Linux kernel technology called Union File System or UnionFS. More specifically, Docker uses a copy-on-write (CoW) implementation of UnionFS, typically leveraging one of the following file system drivers:</p> <ul> <li> <p>aufs (Advanced Multi-Layered Unification File System): It was the original UnionFS implementation used by Docker. However, its support has been deprecated in recent versions of Docker.</p> </li> <li> <p>overlay and overlay2 : These are the current default UnionFS drivers used by Docker. They provide better performance and stability compared to aufs.</p> </li> <li> <p>btrfs (B-Tree File System): Docker can also use btrfs as a file system driver for volumes, but it requires specific configuration and may not be as widely used as overlay or overlay2.</p> </li> </ul> <p>These UnionFS file system drivers allow Docker to create lightweight, isolated, and efficient file system layers for containers. Docker volumes are essentially directories created within the UnionFS that are managed by Docker, providing a separate space for persistent data storage and sharing between containers.  </p> <p>We will learn more about UnionFS and container volumes in Docker images section.</p>"},{"location":"trainings/docker/docker-volume.html#managing-volume","title":"Managing volume","text":"<p>This section includes some essential commands to provide a better understanding of Docker volumes. You may use the official Docker documentation as a reference.  </p> <ul> <li>Run a Nginx container using the below command:  </li> </ul> <pre><code>docker run -it -d -p 8080:80 --name web nginx\n</code></pre> <p>Open a browser and navigate to http://localhost:8080. You should see the following NGINX welcome page.  </p> <p></p> <p>If you want to modify the HTML file, what is the solution? </p> <ul> <li>Adding custom HTML using bind mounting volume:  </li> </ul> <p>Nginx, by default, searches for files to serve in the <code>/usr/share/nginx/html</code> directory within the container. To place our HTML files in this directory, we can utilize a mounted volume. This method allows us to connect a directory on our local machine and map it to the running container.  </p> <p>To serve a custom HTML page using the Nginx image, follow these steps:  </p> <ul> <li>Create a directory called <code>html</code>.</li> <li>Within this directory, add an <code>index.html</code> file.</li> <li>Paste the following HTML content into the <code>index.html</code> file:</li> </ul> <pre><code>&lt;!DOCTYPE html&gt;\n&lt;html&gt;\n&lt;head&gt;\n    &lt;title&gt;CloudyNotes&lt;/title&gt;\n&lt;/head&gt;\n&lt;body&gt;\n    &lt;h1&gt;Welcome to CloudyNotes.io&lt;/h1&gt;\n    &lt;p&gt;This is a Nginx container with a bind volume.&lt;/p&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <ul> <li>Run a new Nginx container with a bind volume: </li> </ul> <pre><code>docker run -it -d -p 8080:80 --name web -v &lt;path_to_html&gt;:/usr/share/nginx/html nginx\n</code></pre> <p>You should remove the other Nginx first.  </p> <p>Replace  to the path of the html directory. <p>Now open the browser and navigate to http://localhost:8080 and you should see the above html rendered in your browser window:  </p> <p></p> <p>Try to modify the <code>index.html</code> and refresh the browser again. Are your changes applying on the ouput? </p> <ul> <li>Inspect the container and find the binding details:  </li> </ul> <pre><code>docker inspect web | grep Binds -A 5 -B 5\n</code></pre> <ul> <li>Now let's try to create a Docker volume:  </li> </ul> <pre><code>docker volume create my_volume\n</code></pre> <ul> <li>Test the volume by adding a file to it:  </li> </ul> <pre><code>echo \"Hello, Docker volume!\" | docker run -i --rm -v my_volume:/app busybox sh -c 'cat &gt; /app/test.txt'\n</code></pre> <p>This command creates a file named \"test.txt\" inside the volume using a Busybox container.</p> <ul> <li>Inspect the Docker volume:  </li> </ul> <pre><code>docker inspect my_volume\n</code></pre> <ul> <li>Navigate to the <code>Mountpoint</code> and check the content of the volume:  </li> </ul> <pre><code>sudo su\ncd /var/lib/docker/volumes/my_volume/_data\nls\n</code></pre>"},{"location":"trainings/docker/docker-volume.html#practice-your-knowledge","title":"Practice your knowledge","text":"<p>You can practice these tasks in a terminal or command prompt with Docker installed on your machine. Make sure you have Docker properly installed and configured before starting the practice scenario.</p> <p>Remember to refer to Docker's official documentation for detailed information on each command and its usage. Happy learning!  </p>"},{"location":"trainings/docker/docker-volume.html#scenario1","title":"Scenario1","text":"<p>Sharing Docker Volume Between Containers</p> <p>In this scenario, you will practice creating a Docker volume, attaching it to two containers, and testing the sharing of the volume between the containers.  </p> <p>Task 1: Create Docker volume</p> <p>Create a new Docker volume named <code>my_volume</code>.</p> <p>Task 2: Run containers with same volume </p> <p>Run the first container named <code>container1</code> using the Nginx image. Attach the <code>my_volume</code> volume to the <code>/app</code> directory inside the container.</p> <p>Run the second container named <code>container2</code> using the Ubuntu image. Attach the same <code>my_volume</code> volume to the <code>/app</code> directory inside the container.</p> <p>Task 3: Verify volume sharing </p> <p>From <code>container1</code>, create a file named <code>test.txt</code> inside the volume with the content <code>Hello from container1</code>. </p> <p>Hint: <code>docker exec container1 sh -c 'echo \"Hello from container1\" &gt; /app/test.txt'</code></p> <p>Verify that the file is accessible from <code>container2</code> and display its content.</p> <p>Update the file from <code>container2</code> with the content <code>Hello from container2</code>.</p> <p>Verify that the changes are reflected when accessing the file from <code>container1</code>.</p> <p>There are several ways to share data between containers. The above scenario was an example of file sharing using a bind volume. You can find more information here.</p>"},{"location":"trainings/docker/intro.html","title":"Introduction to Docker","text":""},{"location":"trainings/docker/intro.html#the-birth-of-the-docker","title":"The birth of the Docker","text":"<p>Docker was first introduced to the world\u2014with no pre-announcement and little fanfare\u2014by Solomon Hykes, founder and CEO of dotCloud, in a five-minute lightning talk at the Python Developers Conference in Santa Clara, California, on March 15, 2013. At the time of this announcement, only about 40 people outside dotCloud been given the opportunity to play with Docker. It didn't take much time for the project to become famous among tech enthusiasts, and many developers started contributing to the project. It sparked a revolution in the field of software development. Docker is a tool that promises to easily encapsulate the process of creating a distributable artifact for any application, deploying it at scale into any environment, and streamlining the workflow and responsiveness of agile software organizations.  </p> <p>Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker\u2019s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production. (https://docs.docker.com/)  </p> <p></p>"},{"location":"trainings/docker/intro.html#why-docker","title":"Why Docker","text":"<p>There are many reasons why Docker became popular in software development. To me, simplicity and cross-platform deployment are the main reasons. Docker simplifies architectural decisions because all applications essentially appear the same from the hosting system's perspective. Additionally, Docker makes tooling easier to write and share between applications. Here are some more of the things you get with Docker:  </p> <ul> <li>Fast, consistent delivery of your applications  </li> <li>Packaging software in a way that leverages the skills developers already have  </li> <li>Bundling application software and required OS filesystems together in a single standardized image format  </li> <li>Running more workloads on the same hardware  </li> <li>Using packaged artifacts to test and deliver the exact same artifact to all systems in all environments  </li> </ul>"},{"location":"trainings/docker/intro.html#process-simplification","title":"Process Simplification","text":"<p>Docker can simplify both workflows and communication, and that usually starts with the deployment story. Traditionally, the cycle of getting an application to production often looks something like the following:  </p> <p> </p> <p>Our experience has shown that deploying a brand new application into production can take the better part of a week for a complex new system. That\u2019s not very productive, and even though DevOps practices work to alleviate some of the barriers, it often requires a lot of effort and communication between teams of people. This process can often be both technically challenging and expensive, but even worse, it can limit the kinds of innovation that development teams will undertake in the future. If deploying software is hard, time-consuming, and requires resources from another team, then developers will often build everything into the existing application in order to avoid suffering the new deployment penalty.  </p> <p>Docker preaches an approach of \u201cbatteries included but removable.\u201d Which means that they want their tools to come with everything most people need to get the job done, while still being built from interchangeable parts that can easily be swapped in and out to support custom solutions. By using an image repository as the hand-off point, Docker allows the responsibility of building the application image to be separated from the deployment and operation of the container.  </p> <p>What this means in practice is that development teams can build their application with all of its dependencies, run it in development and test environments, and then just ship the exact same bundle of application and dependencies to production. Because those bundles all look the same from the outside, operations engineers can then build or install standard tooling to deploy and run the applications.  </p> <p> </p>"},{"location":"trainings/docker/intro.html#what-docker-isnt","title":"What Docker isn't","text":"<p>Docker can be used to solve a wide breadth of challenges that other categories of tools have traditionally been enlisted to fix; however, Docker\u2019s breadth of features often means that it lacks depth in specific functionality. In the following list, we explore some of the tool categories that Docker doesn\u2019t directly replace but that can often be used in conjunction to achieve great results:  </p> <ul> <li> <p>Virtualization Platform A container is not a virtual machine in the traditional sense. Virtual machines contain a complete operating system, running on top of the host operating system. The biggest advantage is that it is easy to run many virtual machines with radically different operating systems on a single host. With containers, both the host and the containers share the same kernel. This means that containers utilize fewer system resources, but must be based on the same underlying operating system. </p> </li> <li> <p>Cloud Platform Like virtualization, the container workflow shares a lot of similarities on the surface with cloud platforms. Both are traditionally leveraged to allow applications to be horizontally scaled in response to changing demand. Docker, however, is not a cloud platform. It only handles deploying, running, and managing containers on pre-existing Docker hosts. It doesn\u2019t allow you to create new host systems (instances), object stores, block storage, and the many other resources that are typically associated with a cloud platform.  </p> </li> </ul> <p> </p>"},{"location":"trainings/docker/intro.html#docker-high-level-design","title":"Docker High-level Design","text":"<p>Docker is a powerful technology, and that often means something that comes with a high level of complexity. But the fundamental architecture of Docker is a simple client/server model, with only one executable that acts as both components, depending on how you invoke the docker command. Underneath this simple exterior, Docker heavily leverages kernel mechanisms such as iptables, virtual bridging, cgroups, namespaces, and various filesystem drivers.  </p> <p>Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. </p> <p> </p>"},{"location":"trainings/docker/intro.html#the-docker-daemon","title":"The Docker daemon","text":"<p>The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.</p>"},{"location":"trainings/docker/intro.html#the-docker-client","title":"The Docker client","text":"<p>The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.  </p>"},{"location":"trainings/docker/intro.html#docker-engine-api","title":"Docker Engine API","text":"<p>Docker provides an API for interacting with the Docker daemon (called the Docker Engine API), as well as SDKs for Go and Python. The SDKs allow you to build and scale Docker apps and solutions quickly and easily. If Go or Python don\u2019t work for you, you can use the Docker Engine API directly.  </p> <p>The Docker Engine API is a RESTful API accessed by an HTTP client such as wget or curl, or the HTTP library which is part of most modern programming languages.  </p>"},{"location":"trainings/docker/intro.html#docker-registries","title":"Docker registries","text":"<p>A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry.</p>"},{"location":"trainings/docker/intro.html#docker-objects","title":"Docker objects","text":"<p>When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects.</p>"},{"location":"trainings/docker/intro.html#images","title":"Images","text":"<p>An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run.</p> <p>You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.</p>"},{"location":"trainings/docker/intro.html#containers","title":"Containers","text":"<p>A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.</p> <p>By default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container\u2019s network, storage, or other underlying subsystems are from other containers or from the host machine.</p> <p>A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear.</p>"},{"location":"trainings/docker/nextgenbts.html","title":"NextGenBTS","text":""},{"location":"trainings/docker/nextgenbts.html#nextgenbts-overview","title":"NextGenBTS Overview","text":"<p>NextGenBTS is an innovative project that leverages Docker to develop a demo BTS application. This training structure provides a hands-on experience in working with Docker and implementing NextGenBTS.  </p> <p>The application consists of three components:  </p> <p> </p> <ul> <li>Core: Python flask application More info </li> <li>Secret manager: HashiCorp Vault More info </li> <li>Database: MongoDB Database More info </li> </ul>"},{"location":"trainings/docker/nextgenbts.html#connecting-to-lab","title":"Connecting to lab","text":"<p>Connecting to the lab is an optional step, if you don't want to run the demo on your environment. </p> <p>To connecting to the lab platform, you should have the SSH key. The key is being shared during the session. Use the below commands to connect to the lab:  </p> <pre><code>chmod 400 lab.pem\nssh -i \"lab.pem\" -o IdentitiesOnly=yes ubuntu@&lt;IP&gt;\n</code></pre> <p>Instaructure is sharing the screen with tmux. Check the active <code>tmux</code> sessions using the below command:  </p> <pre><code>tmux ls\n</code></pre> <p>A session called <code>lab</code> is ready to attach. You should attach to the session in read-only mode using the below command: </p> <pre><code>tmux attach-session -t lab -r\n</code></pre> <p>To exit the session use <code>Ctr + b + d</code></p>"},{"location":"trainings/docker/nextgenbts.html#install-docker","title":"Install Docker","text":"<p>Docker Engine is available on a variety of Linux distros, macOS, and Windows 10 through Docker Desktop, and as a static binary installation. Find your preferred operating system and follow the structure based on official Docker installation guide.</p> <p>The lab platform is based on Ubuntu version. The following guides show how to install Docker on Ubuntu step-by-step. Ref: Install Docker Engine on Ubuntu</p>"},{"location":"trainings/docker/nextgenbts.html#uninstall-old-versions","title":"Uninstall old versions","text":"<p>Older versions of Docker went by the names of docker, docker.io, or docker-engine, you might also have installations of containerd or runc. Uninstall any such older versions before attempting to install a new version:  </p> <pre><code>for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg; done\n</code></pre>"},{"location":"trainings/docker/nextgenbts.html#set-up-the-repository","title":"Set up the repository","text":"<p>Update the apt package index and install packages to allow apt to use a repository over HTTPS:  </p> <pre><code>sudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\n</code></pre> <p>Add Docker\u2019s official GPG key:  </p> <pre><code>sudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n</code></pre> <p>Use the following command to set up the repository:</p> <pre><code>echo \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre>"},{"location":"trainings/docker/nextgenbts.html#install-docker-engine","title":"Install Docker Engine","text":"<p>Update the apt package index:</p> <pre><code>sudo apt-get update\n</code></pre> <p>Install the latest version of Docker Engine, containerd, and Docker Compose. </p> <pre><code>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-compose\n</code></pre>"},{"location":"trainings/docker/nextgenbts.html#post-installation-steps","title":"post-installation steps","text":"<p>These optional post-installation procedures shows you how to configure your Linux host machine to work better with Docker. The Docker daemon binds to a Unix socket, not a TCP port. By default it\u2019s the root user that owns the Unix socket, and other users can only access it using sudo. The Docker daemon always runs as the root user.</p> <p>If you don\u2019t want to preface the docker command with sudo, create a Unix group called docker and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group. On some Linux distributions, the system automatically creates this group when installing Docker Engine using a package manager. In that case, there is no need for you to manually create the group.  </p> <p>Create the docker group:</p> <pre><code>sudo groupadd docker\n</code></pre> <p>Add your user to the docker group:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>Reboot the system:</p> <pre><code>sudo reboot\n</code></pre> <p>After reboot you should follow the steps to connect to the lab and attach to the session.</p> <p>We can verify the installtion by running:  </p> <pre><code>docker system info\n</code></pre> <p>Which information does the command provide to us?  </p> <p>Docker provides a script to install Docker automatically: get.docker.com Run this command to install Docker automatically: <code>curl -fsSL https://get.docker.com | sh</code></p>"},{"location":"trainings/docker/nextgenbts.html#setup-nextgenbts","title":"Setup NextGenBTS","text":""},{"location":"trainings/docker/nextgenbts.html#downloading-and-setup","title":"Downloading and Setup","text":"<p>To download and set up the NextGenBTS project, follow these steps:  </p> <p>Clone the NextGenBTS Repository:</p> <pre><code>git clone https://github.com/meraj-kashi/NextGenBTS.git\n</code></pre> <p>Navigate to the cloned repository directory:</p> <pre><code>cd NextGenBTS/docker-compose\n</code></pre> <p>Run the script to deploy NextGenBTS application:</p> <pre><code>./run.sh\n</code></pre> <p>Verify the NextGenBTS is up and running by navihating to the below url in your browser: </p> <pre><code>http://&lt;IP&gt;:5000\n</code></pre> <p> </p> <p>Congradulations! You deployed NextGenBTS successfully!</p> <p>You are being provided the usernamer and password to explore the NextGenBTS.</p>"},{"location":"trainings/docker/nextgenbts.html#docker-in-surface","title":"Docker in surface","text":"<p>The below section shows some basic docker commands:  </p> <p>Check the system info again:  </p> <pre><code>docker system info\n</code></pre> <p>Check running containers:  </p> <pre><code>docker ps\n</code></pre> <p>Check local Docker images:  </p> <pre><code>docker image ls\n\nOR\n\ndocker images\n</code></pre> <p>Run the below command to spin up the hello-world container:  </p> <pre><code>docker run hello-world\n</code></pre> <p>Check the output and steps!</p> <p>Now, check available Docker images.  </p> <p>Try to run an Ubuntu Docker container. (Hint: Find the image name from docker hub)  </p> <p>During deployment of Ubuntu container, try to use <code>-i</code> and <code>-t</code> switch. What is the best way to exit the container?  </p> <p>Now, re-run the command and add <code>-d</code> switch. Check number of running containers. </p> <p>Run the below command:  </p> <pre><code>docker ps -a\n</code></pre> <p>Why does it show more containers? Check status of containers!  </p> <p>Try to remove the exited containers using the below command:  </p> <pre><code>docker rm\n</code></pre> <p>Now, try to remove a running container, what is the result of <code>docker rm</code> on a running container?  </p> <p>Before using <code>--force</code> option, try to stop the container. </p> <p>What do you think about deleting Docker images? </p>"},{"location":"trainings/docker/nextgenbts.html#practice-your-knowledge","title":"Practice your knowledge","text":"<p>You can practice these tasks in a terminal or command prompt with Docker installed on your machine. Make sure you have Docker properly installed and configured before starting the practice scenario.</p> <p>Remember to refer to Docker's official documentation for detailed information on each command and its usage. Happy learning!  </p>"},{"location":"trainings/docker/nextgenbts.html#scenario1","title":"Scenario1","text":"<p>Setting up a Web Server Container</p> <p>Task 1: Pull a Docker Image </p> <p>Pull the official Nginx Docker image from the Docker Hub repository using the <code>docker pull</code> command.  </p> <p>Task 2: Run a Container in Interactive Mode </p> <p>Run a new Docker container using the Nginx image with the <code>docker run</code> command in interactive mode (<code>-it</code> flag). Map the container's port <code>80</code> to a port on your local machine (e.g., <code>8080</code>) using the <code>-p</code> flag.  </p> <p>Task 3: Access the Web Server </p> <p>Open a web browser and navigate to <code>http://localhost:8080</code> to access the Nginx web server running inside the Docker container.  </p> <p>Task 4: Attach and Detach from a Running Container </p> <p>While the container is running, detach from the container's console without stopping it using the <code>Ctrl + P, Ctrl + Q</code> key combination.  </p> <p>Task 5: Execute Commands Inside the Container </p> <p>Reattach to the running container's console using the <code>docker attach</code> command. Once inside the container, execute commands like <code>ls</code>, <code>pwd</code>, etc., to explore the container's filesystem. Exit the container's console by typing exit.  </p> <p>Task 6: Copy Files to and from a Container </p> <p>Copy a file from your local machine to the running container using the <code>docker cp</code> command. Copy a file from the container to your local machine using the <code>docker cp</code> command.</p> <p>Task 7: Stop and Start a Container </p> <p>Stop the running container using the <code>docker stop</code> command. Start the stopped container again using the <code>docker start</code> command.  </p> <p>Task 8: Run a Container in Detached Mode </p> <p>Run a new Docker container in detached mode (<code>-d</code> flag) using the Nginx image. Map the container's port <code>80</code> to a port on your local machine (e.g., <code>8081</code>) using the <code>-p</code> flag.  </p>"},{"location":"trainings/docker/nextgenbts.html#scenario2","title":"Scenario2","text":"<p>Docker exec vs. Docker attach </p> <p>Task 1: Run a Container with a Specific Name </p> <ol> <li>Run a new Docker container from the Ubuntu image in detached mode (<code>-d</code> flag).</li> <li>Add a specific name to the container using the <code>--name</code> flag (e.g., \"my-container\").  </li> </ol> <p>Task 2: View Container Processes </p> <ol> <li>Use the <code>docker ps</code> command to view the running containers.</li> <li>Identify the process ID (PID) of the container named \"my-container\".  </li> </ol> <p>Task 3: Execute Commands Inside the Container with <code>docker exec</code> </p> <ol> <li>Use the <code>docker exec</code> command to execute a command (e.g., <code>ls -l</code>) inside the running container named \"my-container\" using the container's PID.</li> <li>Verify that the command is executed successfully and observe the output.  </li> </ol> <p>Task 4: Attach to the Container Console with <code>docker attach</code> </p> <ol> <li>Use the <code>docker attach</code> command to attach to the running container named \"my-container\".</li> <li>Inside the attached console, execute a command (e.g., <code>ls</code>) and observe the output.</li> <li>Detach from the container's console without stopping it using the <code>Ctrl + P, Ctrl + Q</code> key combination.  </li> </ol> <p>Task 5: Delete a Container </p> <ol> <li>Stop the running container named \"my-container\" using the <code>docker stop</code> command.</li> <li>Delete the stopped container using the <code>docker rm</code> command, specifying the container's name or ID.  </li> </ol> <p>Task 6: Delete an Image </p> <ol> <li>List all the Docker images on your system using the <code>docker images</code> command.</li> <li>Identify the ID of the image you want to delete.</li> <li>Delete the image using the <code>docker rmi</code> command, specifying the image's ID.  </li> </ol> <p>Understanding the differences between <code>docker exec</code> and <code>docker attach</code> will help you work with containers more effectively. Additionally, managing containers and images is crucial for maintaining your Docker environment.</p>"},{"location":"trainings/docker/private-registry.html","title":"Private Registry","text":""},{"location":"trainings/docker/private-registry.html#intro","title":"Intro","text":"<p>Docker registry is a crucial component in the Docker ecosystem that serves as a storage and distribution platform for Docker container images. It allows developers and organizations to store, share, and manage Docker images securely and efficiently.</p> <p>When you create a Docker image, it consists of layers that make up the file system for your container. These images can be quite large, and sharing them directly between developers or deploying them on various machines can be cumbersome. Docker registry provides a centralized and scalable solution for storing and sharing these images.</p> <p>The Docker registry can be either a public registry, such as Docker Hub, which allows users to share images publicly, or it can be a private registry set up by organizations for internal use or to secure their proprietary images.</p> <p>Private Docker registries offer several compelling reasons for organizations to use them instead of relying solely on public registries like Docker Hub. Some of the key reasons include:</p> <ol> <li> <p>Security and Control: Private registries allow organizations to have full control over their container images and who can access them. This is particularly important for sensitive or proprietary applications where the code and configurations need to be tightly controlled.</p> </li> <li> <p>Intellectual Property Protection: Companies often have valuable intellectual property contained within their Docker images. A private registry ensures that these images are only accessible to authorized users, minimizing the risk of data leaks or unauthorized usage.</p> </li> <li> <p>Compliance and Data Privacy: In regulated industries or regions with strict data privacy laws, hosting sensitive data in public registries might not be compliant. Private registries enable organizations to adhere to regulatory requirements and keep their data within specific jurisdictions.</p> </li> <li> <p>Performance and Bandwidth: Hosting large-scale Docker deployments might require significant bandwidth for image pulls. Private registries hosted within an organization's infrastructure can offer faster image retrieval and reduce reliance on external networks.</p> </li> <li> <p>Offline Access: Private registries allow developers and systems to work with Docker images even when not connected to the internet, making it more convenient for internal development and testing.</p> </li> <li> <p>Customization and Custom Plugins: Organizations can customize and extend private registries to meet their specific needs. They can integrate with existing authentication systems, implement custom security measures, and develop plugins to enhance functionality.</p> </li> <li> <p>Dependency Management: Public registries might experience changes or removal of images, leading to unexpected issues for applications reliant on those images. A private registry can help manage dependencies more effectively by hosting specific versions of required images.</p> </li> <li> <p>Quality Control: In a private registry, an organization can perform thorough testing and validation before making an image available for deployment. This ensures that only stable and tested images are used in production environments.</p> </li> <li> <p>Reduced Vulnerabilities: Using public images from unknown sources can pose security risks. With a private registry, organizations can curate a set of trusted images, reducing the chance of deploying vulnerable or malicious containers.</p> </li> <li> <p>Network Isolation: In certain scenarios, network isolation might be necessary. Private registries can be deployed within specific network boundaries, making it easier to control access and maintain security.</p> </li> </ol>"},{"location":"trainings/docker/private-registry.html#installing-and-configuring","title":"Installing and configuring","text":"<p>This procedure provides basic installation and configuration instructions for learning purposes and should not be used for production.</p> <p>Below is a step-by-step procedure to install and configure a private Docker registry using the open-source distribution of Docker called Docker CE (Community Edition). We'll assume you are setting up the registry on a Linux-based system. </p> <p>Make sure you have Docker installed on your system. If you don't have Docker already installed, you can follow the official Docker installation instructions for your Linux distribution: https://docs.docker.com/engine/install/.  </p> <p>First let's create a file contains the registry username and password:</p> <pre><code>mkdir auth\n</code></pre> <p>You can obtain the htpasswd utility by installing the apache2-utils package. Do so by running: </p> <pre><code>sudo apt install apache2-utils -y\n</code></pre> <p>Create a user, replacing username with the username you want to use. The <code>-B</code> flag orders the use of the bcrypt algorithm, which Docker requires:</p> <pre><code>htpasswd -Bc registry.password &lt;username&gt;\n</code></pre> <p>Now, let's run the Docker registry container command using the auth/htpasswd authentication file:</p> <pre><code>docker run -itd \\\n  -p 5000:5000 \\\n  --name registry \\\n  -v \"$(pwd)\"/auth:/auth \\\n  -e \"REGISTRY_AUTH=htpasswd\" \\\n  -e \"REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm\" \\\n  -e REGISTRY_AUTH_HTPASSWD_PATH=/auth/registry.password \\\n  registry\n</code></pre> <p>To verify the Docker registry, you can navigate to the <code>http://localhost:5000/v2</code>. </p>"},{"location":"trainings/docker/private-registry.html#pushing-to-private-registry","title":"Pushing to private registry","text":"<p>Now that your Docker Registry server is running, you can try pushing an image to it. Docker, by default, attempts to push and pull from Docker Hub. Therefore, you need to log in to the new registry before using it.</p> <pre><code>docker login localhost:5000\n</code></pre> <p>The login has failed because we did not set up SSL/TLS for the private registry. To add security, you can generate a self-signed SSL certificate to secure your private registry. For the simplicity of the test, we can configure the Docker engine to deploy a plain HTTP registry. read more here!</p> <p>This procedure configures Docker to entirely disregard security for your registry. This is very insecure and is not recommended. It exposes your registry to trivial man-in-the-middle (MITM) attacks. Only use this solution for isolated testing or in a tightly controlled, air-gapped environment.</p> <p>Edit the <code>daemon.json</code> file, whose default location is <code>/etc/docker/daemon.json</code>. If the daemon.json file does not exist, create it. Assuming there are no other settings in the file, it should have the following contents:</p> <pre><code>{\n  \"insecure-registries\" : [\"localhost:5000\"]\n}\n</code></pre> <p>Replace ip/url with the proper registry address/IP.</p> <p>Now try to login again: </p> <pre><code>docker login localhost:5000\n</code></pre> <p>You can simply push images to your private Docker registry:</p> <pre><code>docker push localhost:5000/my_image\n</code></pre>"},{"location":"trainings/docker/under-the-hood.html","title":"Under the hood of Docker","text":"<p>We have completed the initial warm-up level and have gained some familiarity with Docker at a basic level. Now, it's time to take a deeper dive into Docker and explore the underlying technologies that power it.</p>"},{"location":"trainings/docker/under-the-hood.html#recap","title":"Recap","text":"<p>Let's take a look at a commonly used design that illustrates the differences between virtualization technology and Docker.  </p> <p> </p> <p>Building upon our previous hands-on experience, we observed that container functionality bears a striking resemblance to that of a virtual machine (VM). Try to deploy an Ubuntu container again:  </p> <pre><code>docker run -it ubuntu\n</code></pre> <p>The deployment and execution of Ubuntu were remarkably fast! Run <code>uname -a</code> locally and in container. Compare ouputs!</p> <p>We can observe that the prompt shell has changed to <code>root@&lt;id&gt;</code>, indicating that it is no longer the local prompt of your system. we can check all the running process. There is only one running process which is the current bash with the <code>PID=1</code>.  </p> <pre><code>ps aux\n</code></pre> <p>Run the same command locally. Which command is running with <code>PID=1</code>?   </p> <p>Check the network interfaces:  </p> <pre><code>ip a\n</code></pre> <p>The <code>ip</code> command is not installed by default, so run <code>apt update</code>, and then <code>apt install iproute2</code>.   </p> <p>There are only two ethernet interfaces; <code>LO</code>(Loopback) and <code>eth#</code>. You can see that <code>eth#</code> ip range is different from your local host ip range!  </p> <p>As a final observation, we are installing the Nginx web server and run it in Linux service:  </p> <pre><code>apt update\napt install nginx -y\nservice nginx start\n</code></pre> <p>and check the service status:  </p> <pre><code>service nginx status\n</code></pre> <p>Did you see that we did not use <code>systemctl</code> command to run the Nginx service?  </p> <p>If we deploy an Ubuntu VM on VirtualBox (or any other virtualization platform), we would have almost the same functionalities as the above Ubuntu container.  In a VERY high-level approach, containers can be considered as lightweight VMs (Docker experts are shouting now).  </p>"},{"location":"trainings/docker/under-the-hood.html#low-level-approach","title":"Low-level approach","text":"<p>If you check the hints in the above tests, you can see that a Docker container uses the host kernel and doesn't need <code>init</code> as <code>PID=1</code>. So, although a Docker container's functionality is similar to a VM, a container is just a process on its host.  </p> <p>Run <code>docker ps</code> and find the ubuntu container id. Now, in your local machine, run <code>ps aux | grep &lt;container_id&gt;</code>.  </p> <p>Docker utilizes several Linux functionalities under the hood. In the following section, we will briefly walk through some of those.  </p>"},{"location":"trainings/docker/under-the-hood.html#capabilities","title":"Capabilities","text":"<p>For the purpose of performing permission checks, traditional UNIX implementations distinguish two categories of processes: privileged processes (whose effective user ID is 0, referred to as superuser or root), and unprivileged processes (whose effective UID is nonzero).  Privileged processes bypass all kernel permission checks, while unprivileged processes are subject to full permission checking based on the process's credentials (usually: effective UID, effective GID, and supplementary group list).  </p> <p>Starting with kernel 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled.  Capabilities are a per-thread attribute. (Ref: Linux man page)  </p> <p>Capabilities apply to both files and threads. File capabilities allow users to execute programs with higher privileges. This is similar to the way the setuid bit works. Thread capabilities keep track of the current state of capabilities in running programs.  </p> <p>In an environment without file based capabilities, it\u2019s not possible for applications to escalate their privileges beyond the bounding set (a set beyond which capabilities cannot grow). Docker sets the bounding set before starting a container. You can use Docker commands to add or remove capabilities to or from the bounding set. By default, Docker drops all capabilities except [those needed] using a whitelist approach. Learn more about Docker and Linux capabilities here.</p>"},{"location":"trainings/docker/under-the-hood.html#cgroups","title":"Cgroups","text":"<p>Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups whose usage of various types of resources can then be limited and monitored.  The kernel's cgroup interface is provided through a pseudo-filesystem called cgroupfs.  Grouping is implemented in the core cgroup kernel code, while resource tracking and limits are implemented in a set of per-resource-type subsystems (memory, CPU, and so on).  </p> <p>A cgroup is a collection of processes that are bound to a set of limits or parameters defined via the cgroup filesystem. A subsystem is a kernel component that modifies the behavior of the processes in a cgroup.  Various subsystems have been implemented, making it possible to do things such as limiting the amount of CPU time and memory available to a cgroup, accounting for the CPU time used by a cgroup, and freezing and resuming execution of the processes in a cgroup.  Subsystems are sometimes also known as resource controllers (or simply, controllers).  </p> <p>The cgroups for a controller are arranged in a hierarchy.  This hierarchy is defined by creating, removing, and renaming subdirectories within the cgroup filesystem.  At each level of the hierarchy, attributes (e.g., limits) can be defined.  The limits, control, and accounting provided by cgroups generally have effect throughout the subhierarchy underneath the cgroup where the attributes are defined.  Thus, for example, the limits placed on a cgroup at a higher level in the hierarchy cannot be exceeded by descendant cgroups.  (Ref: Linux man page)  </p> <pre><code>docker ps\nps aux | grep &lt;container-id&gt;\ncat /proc/[pid]/cgroup\n</code></pre> <p>For each cgroup hierarchy of which the process is a member, there is one entry containing three colon-separated fields:</p> <p>hierarchy-ID::cgroup-path</p> <ul> <li>hierarchy-ID: For the cgroups version 2 hierarchy, this field contains the value 0. </li> <li>cgroup-path: This field contains the pathname of the control group in the hierarchy to which the process belongs.  This pathname is relative to the mount point of the hierarchy.</li> </ul> <pre><code>docker info | grep containerd\n</code></pre> <p>More info about containerd: https://containerd.io/</p> <p> </p> <p>more details about Linux cgroups:</p> <pre><code>systemd-cgls\n</code></pre> <pre><code>systemd-cgtop\n</code></pre> <pre><code>docker stats \n</code></pre> <p>Learn more about Docker and Linux Cgroups here.</p>"},{"location":"trainings/docker/under-the-hood.html#namespaces","title":"Namespaces","text":"<p>Namespaces are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources while another set of processes sees a different set of resources.  </p> <p>Starting from kernel version 5.6 onwards, there exist eight different types of namespaces. Regardless of the type, the functionality of namespaces remains consistent. Each process is linked to a specific namespace, restricting its access to only the resources associated with that particular namespace and any relevant descendant namespaces. As a result, every process or group of processes can have a distinct perspective on the available resources. The specific resource that is isolated depends on the type of namespace created for a particular process group. The below list shows different type of namespaces:  </p> <ul> <li>Mount (mnt)  </li> <li>Process ID (pid)  </li> <li>Network (net)  </li> <li>Inter-process Communication (ipc)  </li> <li>UNIX Time-Sharing (UTS)  </li> <li>User ID (user)  </li> <li>Control group (cgroup)  </li> <li>Time Namespace  </li> </ul> <p>To see the namespace example, pull the NextGenBTS project: (<code>https://github.com/meraj-kashi/NextGenBTS</code>), navigate to <code>dev/namespace/network</code>. Build the project:  </p> <pre><code>gcc -o network_namespace network_namespace.c\n</code></pre> <p>run the code and check ethernet interfaces:  </p> <pre><code>./network_namespace\n</code></pre>"},{"location":"trainings/docker/under-the-hood.html#seccom","title":"Seccom","text":"<p>Seccomp, short for Secure Computing Mode, is a feature in the Linux kernel that enhances multiple security aspects, providing a more secure environment for running Docker. More information here.  </p>"},{"location":"trainings/docker/under-the-hood.html#apparmor","title":"AppArmor","text":"<p>AppArmor is a security module integrated into the Linux kernel, which enables system administrators to impose restrictions on the capabilities of programs through per-program profiles. These profiles define what actions a program is allowed to perform, such as granting network access, raw socket access, and read, write, or execute permissions for specific file paths. Check this blog for more information.</p>"},{"location":"trainings/docker/under-the-hood.html#demo","title":"Demo","text":"<pre><code>#First mount a btrfs disk:\nmkfs.btrfs /dev/sdb1\nmkdir /container\nmount /dev/sdb1 /container\nmount | grep btrfs\n\nmount --make-rprivate / \nmkdir -p images containers\nbtrfs subvol create images/alpine\nwget https://dl-cdn.alpinelinux.org/alpine/v3.18/releases/x86_64/alpine-minirootfs-3.18.0-x86_64.tar.gz\ntar -C images/alpine/ -xf alpine-minirootfs-3.18.0-x86_64.tar.gz\nbtrfs subvol snapshot images/alpine/ containers/nextgenbts\nchroot containers/nextgenbts/ sh\nexit\nunshare --mount --uts --ipc --net --pid --fork bash\nhostname nextgenbts\nexec bash\nps\npidof unshare\nkill &lt;pid&gt;\nmount -t proc none /proc\nps aux\nip a\n</code></pre>"}]}