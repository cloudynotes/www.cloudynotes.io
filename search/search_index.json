{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"blog/camelup.html","title":"Yo-Yo attacks on cloud auto-scaling","text":"","tags":["Cloud"]},{"location":"blog/main.html","title":"blogs","text":"<p>This blog section is a place where we can explore a wide range of topics and ideas, from the latest technology trends to tips for self-improvement and personal growth. Whether you're a tech enthusiast, a curious learner, or someone seeking inspiration, there's something for everyone here.</p> <p>{{ blog_content }}</p> <p>Thank you for joining me on this adventure, and I look forward to connecting with you soon!  </p>"},{"location":"blog/rulebook.html","title":"Unlocking the Power of Effective Documentation","text":"<p>Good documentation for a product is like a well-written rulebook in a boardgame - it makes the game easy to understand, enjoyable to play, and accessible to all.</p> <p>Root is a strategy board game that took the tabletop world by storm upon its release in 2018. Developed by Cole Wehrle and published by Leder Games, Root is a game of woodland might and right that pits players against each other in a battle for control over a richly detailed and immersive woodland kingdom. With its unique asymmetric gameplay, gorgeous artwork, and rich thematic setting, Root offers endless hours of fun and strategic challenge for players of all skill levels. Whether you're a seasoned tabletop enthusiast or a newcomer to the world of board games, Root is sure to captivate your imagination and test your strategic abilities to their limits. Although Root is a complex game with a rating of 3.78/5, a well-designed rulebook makes it easier to learn and start playing. For more information, please visit: BoardGameGeek</p>","tags":["General"]},{"location":"blog/rulebook.html#introduction","title":"Introduction","text":"<p>While a board game may be well-designed and not overly complex, it can be difficult to understand and enjoy without a good rulebook. Developers often focus on writing efficient, high-quality code that meets the requirements of the project. However, the importance of good documentation is sometimes overlooked. Good documentation is an essential component of any product, as it provides a roadmap for users to effectively understand and use it.  </p> <p>Recently, I had a discussion with a colleague about improving the technical documentation for our project, and he suggested checking out divio. The website, written by Daniele Procida, is based on <code>The Grand Unified Theory of Documentation</code> by David Laing, a popular and transformative documentation authoring framework. I have read the websites' articles and watched a YouTube video presented by Daniele. In this blog post, I will provide a summary of the suggested framework, as well as share my personal experience about effective documentation.    </p> <p>It doesn\u2019t matter how good your product is, because if its documentation is not good enough, people will not use it.  documentation.divio.com </p> <p>According to <code>the Grand Unified Theory of Documentation</code> framework, there are four distinct components of documentation, rather than one monolithic entity:  </p> <ul> <li>Tutorials  </li> <li>How-to guide  </li> <li>Reference materials  </li> <li>Discussions  </li> </ul> <p>By recognizing and addressing each of these components separately, we can create more effective and targeted documentation that meets the needs of both authors and readers.  </p>","tags":["General"]},{"location":"blog/rulebook.html#tutorials","title":"Tutorials","text":"<p>Tutorials are instructional lessons that guide readers through a series of steps to complete a project. They are designed to teach how to do something, rather than just to provide information. As a product owner, it is important to ensure that the end goal of the tutorial is both meaningful and achievable for a beginner user. A well-designed tutorial can help learners make sense of the product, while a poorly executed or missing tutorial can hinder the acquisition of new users. Writing and maintaining tutorials can be time-consuming, but they are essential for helping users to successfully navigate and utilize a project. </p> <p>The list below shows the main points of tutorial documents:    </p> <ul> <li>Writing should be clear and easy to understand for the reader.  </li> <li>Provide a hands-on experience that inspires the reader.  </li> <li>Start with simple concepts and gradually move towards more complex ones.  </li> <li>Make the document enjoyable to read. Avoid starting with complex information that may discourage the reader.  </li> <li>Learning by doing should be emphasized throughout the tutorial.  </li> <li>Ensure the tutorial is repeatable on all platforms, even if it requires more work.  </li> <li>Use concrete examples and specifics rather than generalizations.  </li> <li>The tutorial should focus on practical skills.  </li> <li>Avoid distractions and unnecessary options.  </li> <li>Emphasize the importance of the tutorial for those who are new to the project.  </li> <li>Demonstrate to newcomers that it is possible to learn and that the tutorial is oriented towards learning.  </li> </ul>","tags":["General"]},{"location":"blog/rulebook.html#how-to-guide","title":"How-to guide","text":"<p>The purpose of how-to guides is to provide a step-by-step process for solving real-world problems. They are goal-oriented and provide specific instructions for achieving a particular outcome. For example, how to setup your company VPN, or how to add a new SSH key. Unlike tutorials, how-to guides assume some level of prior knowledge and experience from the user. While tutorials are designed for beginners and cover basic concepts, how-to guides are intended to answer more advanced questions. In the realm of software documentation, how-to guides are typically well-executed and enjoyable to write.  </p> <p>Based on a project experience, how-to guides are sometimes also called <code>runbooks</code>. Each runbook has a specific goal and  provides readers with step-by-step guidance to achieve that goal.  </p> <p>The following list outlines the key points covered in the tutorial documents:    </p> <ul> <li>How-to documentation is problem-oriented.  </li> <li>It focuses on achieving a specific result through a step-by-step guide.  </li> <li>How-to guides are entirely practical.  </li> <li>They are easy to write for technical/product owners.  </li> <li>Readers should have some basic knowledge before starting the guide.  </li> <li>How-to documents can be linked together. Therefore, if one document depends on another, readers can easily navigate between them.  </li> <li>Choose an appropriate title for the guide.  </li> </ul>","tags":["General"]},{"location":"blog/rulebook.html#reference-materials","title":"Reference materials","text":"<p>Reference guides provide technical descriptions of machinery and how to operate it. Unlike how-to guides, reference material focuses solely on information, providing details about functions, fields, attributes, and methods. It may include basic descriptions of how to use the machinery, but it should not attempt to explain basic concepts or provide instructions on how to achieve common tasks. Reference material is straightforward and austere, and may be generated automatically to some extent, but it is never sufficient on its own. For some developers, reference guides are the only kind of documentation they can imagine, assuming that others only need technical information about the software.  </p> <p>We use references every day! API references, Linux commands' references, Python packages'references, and more.</p>","tags":["General"]},{"location":"blog/rulebook.html#discussions","title":"Discussions","text":"<p>Discussions or explanations in documentation clarify and broaden the coverage of a particular topic, providing a higher-level perspective and illuminating the subject matter. Unlike how-to guides or reference material, explanations are understanding-oriented and discursive in nature, often scattered among other sections rather than explicitly created. Discussions can be challenging to create as they require a broader view of the software and a deeper understanding of the subject matter. The division of topics for discussion can sometimes be arbitrary, defined by what the author thinks is a reasonable area to cover at one time rather than a specific task or learning objective.  </p>","tags":["General"]},{"location":"blog/rulebook.html#example","title":"Example","text":"<p>Several websites offer their product documentation using this framework. Let's take a look at one of them: Ubuntu Server documentation!  </p> <p> </p> <p>As we can see, the documentation includes the four main components: Ubuntu Server tutorials: This section of the documentation contains step-by-step tutorials to outline what Ubuntu Server is capable of while helping readers achieve specific aims. The tutorials start with a basic installation guide and continue with a collection of related tutorials and topics that will help readers learn more about Ubuntu Server.  </p> <p>Ubuntu Server how-to guides: The documentation includes a variety of how-to guides that provide readers with specific goals to accomplish. However, it is assumed that readers are already familiar with Ubuntu Server, as the guides may require readers to understand and adapt the steps to fit their specific requirements. While the guides will help readers achieve an end result, they may need to be customized to fit specific needs.  </p> <p>Ubuntu Server explanation guides: This section includes explanatory and conceptual materials aimed at enhancing user comprehension of the functioning and configuration of Ubuntu Server, thereby making it easier to use. The section is divided into three main topics: Software, Network, and Cryptography.    </p> <p>Ubuntu Server reference: This section provides a list of available software and command references. Readers can refer to this documentation to figure out how to interact with different Ubuntu tools and commands.  </p>","tags":["General"]},{"location":"blog/rulebook.html#summary","title":"Summary","text":"<p>Based on the <code>the Grand Unified Theory of Documentation</code> framework, the four distinct components of documentation are interconnected. Tutorials and Discussions are particularly helpful for readers who are just starting to learn, while How-to guides and references are more practical documents that assist in development.  </p> <p> </p> Provided by documentation.divio.com <p>For a software development project, a README file is an essential component, as it serves as a guide to help users understand what the project is about and how to use it. A well-written README file can ensure that a project is more accessible, easier to use, and encourages collaboration among developers. Let's discuss README files in another blog post. ;) </p>","tags":["General"]},{"location":"blog/tags.html","title":"List of all blogs","text":"<p>{{ tag_content }}</p>"},{"location":"blog/tf-mars.html","title":"Moving HashiCorp Terraform state file","text":"<p>This isn't a blog about board games, but HashiCorp Terraform makes me think of the Terraforming Mars board game!</p> <p>Terraforming Mars is a board game designed by Jacob Fryxelius and published by FryxGames in 2016. The game is set in the future, where players take on the role of corporations that work together to terraform Mars and make it habitable for human life. You can check more here: BoardGameGeek</p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#terraform-state-file","title":"Terraform state file","text":"<p>Infrastructure as Code (IaC) enables us to use code to manage infrastructure resources. This approach makes it easier to manage complex infrastructures, reduce manual errors, and increase efficiency.</p> <p>These days HashiCorp Terraform is one of the popular IaC tools. It supports a wide range of Cloud providers and services including AWS, Azure, GCP, K8S, and many others. This enables infrastructure engineers to manage their infrastructure resources in a consistent way, regardless of the cloud provider they are using.</p> <p>Terraform provides a state management mechanism to track the state of the infrastructure resources. This allows us to understand the current state of the infrastructure, identify changes that have been made, and easily make updates. Terraform stores the current state of the infrastructure in a file called tfstate. This state is used by Terraform to map real world resources to the configuration, keep track of metadata, and to improve performance for large infrastructures.</p> <p>This state file is stored locally by default in a file called <code>terraform.tfstate</code>. Terraform utilizes the state file to generate plan and carry out modifications to the infrastructure. Terraform performs a refresh before carrying out any action to update the state with the current state of the infrastructure. That\u2019s why we see <code>Refreshing state\u2026</code> in each Terraform plan output.</p> <pre><code>$ terraform plan\naws_dynamodb_table.dynamodb_locktable: Refreshing state... [id=terraforming-mars-locktable]\naws_s3_bucket.s3_tfstate: Refreshing state... [id=terraforming-mars-tfstate]\n</code></pre> <p>You can read more about Terraform state purpose here: Purpose of Terraform State</p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#terraform-backend","title":"Terraform backend","text":"<p>Terraform enables us to collaborate with other members of our team by using version control systems such as Git. This makes it easier to share infrastructure code, review changes, and ensure that everyone is working on the same version of the infrastructure. However, using a local Terraform state file can be challenging because everyone must make sure to pull the latest tfstate file locally and ensure that nobody else is running Terraform at the same time.</p> <p>To solve this issue, Terraform introduces remote state. Using remote state, the state file can be written to a remote data store. Now, teammates can collaborate on a project without any concern about the latest tfstate file version. Remote state is implemented by a backend or by Terraform Cloud.</p> <p>Terraform supports various types of backends, including AWS S3, Azure Blob Storage, and HashiCorp Consul. These backends provide remote storage for Terraform state files, making it easier to manage infrastructure resources across teams and environments. When using a remote backend, Terraform can read the current state and apply changes to the infrastructure based on that state.</p> <p>However, what if Terraform executed concurrently? Terraform has an ability to lock the state file. Whenever there is a possibility of writing state, the process of state locking occurs automatically. Backends are responsible for providing an API for state locking and state locking is optional.</p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#default-backend","title":"Default backend","text":"<p>Terraform uses a backend called <code>local</code> as the default option, which stores the state data locally as a file on the disk. It means that we do not need to add backend block configuration. For example, the below code block shows the terraform block configured with aws provider:</p> <pre><code>    terraform {\n      required_providers {\n        aws = {\n          source = \"hashicorp/aws\"\n        }\n      }\n    }\n</code></pre> <p>The state file in Terraform is typically stored locally in the current project directory. However, you may wonder how to store the tfstate file in a different location. This can be accomplished by specifying a backend configuration in your Terraform code, which tells Terraform where to store the state file:</p> <pre><code>    terraform {\n      required_providers {\n        aws = {\n          source = \"hashicorp/aws\"\n        }\n      }\n      backend \"local\" {\n        path = \"local_path/terraform.tfstate\"\n      }\n    }\n</code></pre> <p>By adding the backend configuration block and running <code>terraform init</code>, you will get an error message indicates change in backend configuration:</p> <pre><code>    $ terraform init\n    Initializing the backend...\n    Error: Backend configuration changed\n\n    A change in the backend configuration has been detected, which may require migrating existing state.\n\n    If you wish to attempt automatic migration of the state, use \"terraform init -migrate-state\".\n    If you wish to store the current configuration with no changes to the state, use \"terraform init -reconfigure\".\n</code></pre> <p>The error message simply explains the root cause and the possible solutions. The <code>-migrate-state</code> option will attempt to copy existing state to the new backend, and depending on what changed, may result in interactive prompts to confirm migration of workspace states. On the other hand, the <code>-reconfigure</code> option disregards any existing configuration, preventing migration of any existing state.  </p> <p>If you are trying to move the state file from the default working directory to your custom directory, <code>-migrate-state</code> is the correct option.</p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#migrating-to-a-remote-backend","title":"Migrating to a remote backend","text":"<p> Now, how can we move the local state file of a current project to a remote backend? As we understood, using a remote backend can help improve collaboration, scalability, security, and ease of management when working with Terraform.</p> <p>I would like to divide the supported backends into two categories: Local and Remote. In the Local group, the state file is stored locally (default or using a local configuration). The Remote group includes options such as Terraform Cloud, AWS S3, Azurerm, and others.</p> <p>HashiCorp says that remote backend is unique among all other Terraform backends. Read more about it here: Terraform Remote Backend</p> <p>In this demonstration, I try to use AWS S3 backend. AWS S3 backend supports state locking via AWS DynamoDB. It means that it doesn\u2019t support state locking out of the box.</p> <p>As an example of an in-the-box locking feature, Azurerm supports state locking and consistency checking with Azure Blob Storage native capabilities.</p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#implementation","title":"Implementation","text":"<p>In the first step, let's create resources an AWS to support storing Terraform project state file and status. Based on a project experience, I have a project called <code>iac-base</code> includes all the base infrastructure for other projects deployment. The below code block shows <code>iac-base</code> resources:  </p> <pre><code>    # S3 bucket \n    resource \"aws_s3_bucket\" \"s3_tfstate\" {\n      bucket = \"terraforming-mars-tfstate\"\n    }\n\n    # S3 bucket ACL\n    resource \"aws_s3_bucket_acl\" \"s3_acl\" {\n      bucket = aws_s3_bucket.s3_tfstate.id\n      acl    = \"private\"\n    }\n\n    # S3 bucket encryption\n    resource \"aws_s3_bucket_server_side_encryption_configuration\" \"s3_encryption\" {\n      bucket = aws_s3_bucket.s3_tfstate.bucket\n\n      rule {\n        apply_server_side_encryption_by_default {\n          sse_algorithm = \"aws:kms\"\n        }\n      }\n    }\n\n    resource \"aws_s3_bucket_versioning\" \"s3_bucket_versioning\" {\n      bucket = aws_s3_bucket.s3_tfstate.id\n      versioning_configuration {\n        status = \"Enabled\"\n      }\n    }\n\n    resource \"aws_s3_bucket_lifecycle_configuration\" \"s3_bucket_retention_policy\" {\n      bucket     = aws_s3_bucket.s3_tfstate.id\n      depends_on = [aws_s3_bucket_versioning.s3_bucket_versioning]\n\n\n      rule {\n        status = \"Enabled\"\n        id     = \"retention_policy\"\n        noncurrent_version_expiration {\n          noncurrent_days = 180\n        }\n\n      }\n    }\n\n    resource \"aws_s3_bucket_public_access_block\" \"bucket_block_public\" {\n      bucket = aws_s3_bucket.s3_tfstate.id\n\n      block_public_acls       = true\n      block_public_policy     = true\n      ignore_public_acls      = true\n      restrict_public_buckets = true\n    }\n\n    # DynamoDB \n    resource \"aws_dynamodb_table\" \"dynamodb_locktable\" {\n      name           = \"terraforming-mars-locktable\"\n      hash_key       = \"LockID\"\n      billing_mode   = \"PROVISIONED\"\n      write_capacity = 1\n      read_capacity  = 1\n\n      attribute {\n        name = \"LockID\"\n        type = \"S\"\n      }\n    }\n</code></pre> <p>The above code block creats a AWS S3 bucket based on the best practices and a DynamoDB table for state locking. After applying the configuration, your base resources to store projects state files is ready. Now we are ready to migrate projects state file from local to AWS S3 remote backend. Modify your Terraform code block to add AWS remote backend configuration:</p> <pre><code>    terraform {\n      required_providers {\n        aws = {\n          source = \"hashicorp/aws\"\n        }\n      }\n      backend \"s3\" {\n        bucket         = \"terraforming-mars-tfstate\"\n        key            = \"terraform.state\"\n        region         = \"eu-west-1\"\n        encrypt        = true\n        dynamodb_table = \"terraforming-mars-locktable\"\n      }\n    }\n</code></pre> <p>I created base resources in <code>eu-west-1</code> region. You should use the correct region based on your configuration.</p> <p>I also migrate <code>iac-base</code> Terraform state file to this remote backend.</p> <p>Migration from local to a remote backend is EASIER than moving resources from Earth to Mars. You only need to run <code>terraform init</code>. Terraform detects the new backend configuration, and asks about migrating:  </p> <pre><code>    $ terraform init \n\n    Initializing the backend...\n    Do you want to copy existing state to the new backend?\n      Pre-existing state was found while migrating the previous \"local\" backend to the\n      newly configured \"s3\" backend. No existing state was found in the newly\n      configured \"s3\" backend. Do you want to copy this state to the new \"s3\"\n      backend? Enter \"yes\" to copy and \"no\" to start with an empty state.\n\n      Enter a value: yes\n\n    Releasing state lock. This may take a few moments...\n\n    Successfully configured the backend \"s3\"! Terraform will automatically\n    use this backend unless the backend configuration changes.\n\n    Initializing provider plugins...\n    - Reusing previous version of hashicorp/aws from the dependency lock file\n    - Using previously-installed hashicorp/aws v4.57.0\n\n    Terraform has been successfully initialized!\n\n    You may now begin working with Terraform. Try running \"terraform plan\" to see\n    any changes that are required for your infrastructure. All Terraform commands\n    should now work.\n\n    If you ever set or change modules or backend configuration for Terraform,\n    rerun this command to reinitialize your working directory. If you forget, other\n    commands will detect it and remind you to do so if necessary.\n</code></pre> <p>One of the core concepts of the IaC is about writing one time, and use several times. For example, you can use the same resource implementation to deploy in several environments such as Development, Test, Stage, or Production. Then it comes to a concept called multi-account and multi-backend architecture. I will discuss this concept in a separate blog. </p>","tags":["Terraform"]},{"location":"blog/tf-mars.html#changing-the-s3-bucket","title":"Changing the S3 Bucket","text":"<p>I always say it is better to consider all details before implementation. A design document including all the project details can prevent most future issues. For instance, naming conventions is one of my criteria. But it might happen that you should change the state S3 bucket to another bucket. In this case, Terraform can move your state file from one bucket to another bucket using <code>terraform init -migrate-state</code>. In the below code block, I try to move state file from <code>terraforming-mars-tfstate</code> bucket to <code>terraforming-venus-next-tfstate</code>:  </p> <pre><code>    terraform {\n      required_providers {\n        aws = {\n          source = \"hashicorp/aws\"\n        }\n      }\n      backend \"s3\" {\n        bucket         = \"terraforming-venus-next-tfstate\"\n        key            = \"terraform.state\"\n        region         = \"eu-west-1\"\n        encrypt        = true\n        dynamodb_table = \"terraforming-venus-next-locktable\"\n      }\n    }\n</code></pre>","tags":["Terraform"]},{"location":"blog/tf-mars.html#best-practices","title":"Best practices","text":"<p>Enable encryption for S3 bucket: Using encryption for the state file in the S3 bucket. State files can contain secrets, keys, etc. in plaintext. So, it is important to keep it encrypted. AWS S3 backend supports different encryption methods: </p> <ul> <li><code>encrypt</code> - Enable server side encryption of the state file.  </li> <li><code>kms_key_id</code> - Amazon Resource Name (ARN) of a Key Management Service (KMS) Key to use for encrypting the state. Note that if this value is specified, Terraform will need kms:Encrypt, kms:Decrypt and kms:GenerateDataKey permissions on this KMS key.  </li> <li><code>sse_customer_key</code> - The key to use for encrypting state with Server-Side Encryption with Customer-Provided Keys (SSE-C). This is the base64-encoded value of the key, which must decode to 256 bits. This can also be sourced from the <code>AWS_SSE_CUSTOMER_KEY</code> environment variable, which is recommended due to the sensitivity of the value. Setting it inside a terraform file will cause it to be persisted to disk in terraform.tfstate.  </li> </ul> <p>Enable S3 bucket versioning: Enabling bucket versioning on the S3 bucket is strongly advised as it enables recovery of the state in case of unintended deletions and mistakes.  </p> <p>Enable retention lifecycle policy: As S3 bucket versioning enables, it is wise to have a retention lifecycle policy to delete the old state file objects. You can add <code>noncurrent_version_expiration</code> policy based on your project/organization definition.  </p> <p>Suggested structure for single-environment projects: This is only a suggestion based on my experience with how to have a structure for your Terraform projects. As we discussed it is a good idea to have a project called <code>iac-based</code> including all your base configurations. For example, resources for your Terraform backend:</p> <pre><code>$ tree                                                                               \n.\n\u251c\u2500\u2500 iac-base\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 search-planet-x-state.tf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 tf-mars-state.tf\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 tf-venus-state.tf\n\u251c\u2500\u2500 searching-for-planet-x\n\u251c\u2500\u2500 terraforming-mars\n\u2514\u2500\u2500 terraforming-venus\n</code></pre>","tags":["Terraform"]},{"location":"trainings/docker/intro.html","title":"Introduction to Docker","text":""},{"location":"trainings/docker/intro.html#the-birth-of-the-docker","title":"The birth of the Docker","text":"<p>Docker was first introduced to the world\u2014with no pre-announcement and little fanfare\u2014by Solomon Hykes, founder and CEO of dotCloud, in a five-minute lightning talk at the Python Developers Conference in Santa Clara, California, on March 15, 2013. At the time of this announcement, only about 40 people outside dotCloud been given the opportunity to play with Docker. It didn't take much time for the project to become famous among tech enthusiasts, and many developers started contributing to the project. It sparked a revolution in the field of software development. Docker is a tool that promises to easily encapsulate the process of creating a distributable artifact for any application, deploying it at scale into any environment, and streamlining the workflow and responsiveness of agile software organizations.  </p> <p>Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure so you can deliver software quickly. With Docker, you can manage your infrastructure in the same ways you manage your applications. By taking advantage of Docker\u2019s methodologies for shipping, testing, and deploying code quickly, you can significantly reduce the delay between writing code and running it in production. (https://docs.docker.com/)  </p> <p></p>"},{"location":"trainings/docker/intro.html#why-docker","title":"Why Docker","text":"<p>There are many reasons why Docker became popular in software development. To me, simplicity and cross-platform deployment are the main reasons. Docker simplifies architectural decisions because all applications essentially appear the same from the hosting system's perspective. Additionally, Docker makes tooling easier to write and share between applications. Here are some more of the things you get with Docker:  </p> <ul> <li>Fast, consistent delivery of your applications  </li> <li>Packaging software in a way that leverages the skills developers already have  </li> <li>Bundling application software and required OS filesystems together in a single standardized image format  </li> <li>Running more workloads on the same hardware  </li> <li>Using packaged artifacts to test and deliver the exact same artifact to all systems in all environments  </li> </ul>"},{"location":"trainings/docker/intro.html#process-simplification","title":"Process Simplification","text":"<p>Docker can simplify both workflows and communication, and that usually starts with the deployment story. Traditionally, the cycle of getting an application to production often looks something like the following:  </p> <p> </p> <p>Our experience has shown that deploying a brand new application into production can take the better part of a week for a complex new system. That\u2019s not very productive, and even though DevOps practices work to alleviate some of the barriers, it often requires a lot of effort and communication between teams of people. This process can often be both technically challenging and expensive, but even worse, it can limit the kinds of innovation that development teams will undertake in the future. If deploying software is hard, time-consuming, and requires resources from another team, then developers will often build everything into the existing application in order to avoid suffering the new deployment penalty.  </p> <p>Docker preaches an approach of \u201cbatteries included but removable.\u201d Which means that they want their tools to come with everything most people need to get the job done, while still being built from interchangeable parts that can easily be swapped in and out to support custom solutions. By using an image repository as the hand-off point, Docker allows the responsibility of building the application image to be separated from the deployment and operation of the container.  </p> <p>What this means in practice is that development teams can build their application with all of its dependencies, run it in development and test environments, and then just ship the exact same bundle of application and dependencies to production. Because those bundles all look the same from the outside, operations engineers can then build or install standard tooling to deploy and run the applications.  </p> <p> </p>"},{"location":"trainings/docker/intro.html#what-docker-isnt","title":"What Docker isn't","text":"<p>Docker can be used to solve a wide breadth of challenges that other categories of tools have traditionally been enlisted to fix; however, Docker\u2019s breadth of features often means that it lacks depth in specific functionality. In the following list, we explore some of the tool categories that Docker doesn\u2019t directly replace but that can often be used in conjunction to achieve great results:  </p> <ul> <li> <p>Virtualization Platform A container is not a virtual machine in the traditional sense. Virtual machines contain a complete operating system, running on top of the host operating system. The biggest advantage is that it is easy to run many virtual machines with radically different operating systems on a single host. With containers, both the host and the containers share the same kernel. This means that containers utilize fewer system resources, but must be based on the same underlying operating system. </p> </li> <li> <p>Cloud Platform Like virtualization, the container workflow shares a lot of similarities on the surface with cloud platforms. Both are traditionally leveraged to allow applications to be horizontally scaled in response to changing demand. Docker, however, is not a cloud platform. It only handles deploying, running, and managing containers on pre-existing Docker hosts. It doesn\u2019t allow you to create new host systems (instances), object stores, block storage, and the many other resources that are typically associated with a cloud platform.  </p> </li> </ul> <p> </p>"},{"location":"trainings/docker/intro.html#docker-high-level-design","title":"Docker High-level Design","text":"<p>Docker is a powerful technology, and that often means something that comes with a high level of complexity. But the fundamental architecture of Docker is a simple client/server model, with only one executable that acts as both components, depending on how you invoke the docker command. Underneath this simple exterior, Docker heavily leverages kernel mechanisms such as iptables, virtual bridging, cgroups, namespaces, and various filesystem drivers.  </p> <p>Docker uses a client-server architecture. The Docker client talks to the Docker daemon, which does the heavy lifting of building, running, and distributing your Docker containers. The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface. </p> <p> </p>"},{"location":"trainings/docker/intro.html#the-docker-daemon","title":"The Docker daemon","text":"<p>The Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. A daemon can also communicate with other daemons to manage Docker services.</p>"},{"location":"trainings/docker/intro.html#the-docker-client","title":"The Docker client","text":"<p>The Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.  </p>"},{"location":"trainings/docker/intro.html#docker-engine-api","title":"Docker Engine API","text":"<p>Docker provides an API for interacting with the Docker daemon (called the Docker Engine API), as well as SDKs for Go and Python. The SDKs allow you to build and scale Docker apps and solutions quickly and easily. If Go or Python don\u2019t work for you, you can use the Docker Engine API directly.  </p> <p>The Docker Engine API is a RESTful API accessed by an HTTP client such as wget or curl, or the HTTP library which is part of most modern programming languages.  </p>"},{"location":"trainings/docker/intro.html#docker-registries","title":"Docker registries","text":"<p>A Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. You can even run your own private registry.</p>"},{"location":"trainings/docker/intro.html#docker-objects","title":"Docker objects","text":"<p>When you use Docker, you are creating and using images, containers, networks, volumes, plugins, and other objects. This section is a brief overview of some of those objects.</p>"},{"location":"trainings/docker/intro.html#images","title":"Images","text":"<p>An image is a read-only template with instructions for creating a Docker container. Often, an image is based on another image, with some additional customization. For example, you may build an image which is based on the ubuntu image, but installs the Apache web server and your application, as well as the configuration details needed to make your application run.</p> <p>You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. Each instruction in a Dockerfile creates a layer in the image. When you change the Dockerfile and rebuild the image, only those layers which have changed are rebuilt. This is part of what makes images so lightweight, small, and fast, when compared to other virtualization technologies.</p>"},{"location":"trainings/docker/intro.html#containers","title":"Containers","text":"<p>A container is a runnable instance of an image. You can create, start, stop, move, or delete a container using the Docker API or CLI. You can connect a container to one or more networks, attach storage to it, or even create a new image based on its current state.</p> <p>By default, a container is relatively well isolated from other containers and its host machine. You can control how isolated a container\u2019s network, storage, or other underlying subsystems are from other containers or from the host machine.</p> <p>A container is defined by its image as well as any configuration options you provide to it when you create or start it. When a container is removed, any changes to its state that are not stored in persistent storage disappear.</p>"},{"location":"trainings/docker/nextgenbts.html","title":"NextGenBTS","text":""},{"location":"trainings/docker/nextgenbts.html#nextgenbts-overview","title":"NextGenBTS Overview","text":"<p>NextGenBTS is an innovative project that leverages Docker to develop a demo BTS application. This training structure provides a hands-on experience in working with Docker and implementing NextGenBTS.  </p> <p>The application consists of three components:  </p> <p> </p> <ul> <li>Core: Python flask application More info </li> <li>Secret manager: HashiCorp Vault More info </li> <li>Database: MongoDB Database More info </li> </ul>"},{"location":"trainings/docker/nextgenbts.html#connecting-to-lab","title":"Connecting to lab","text":"<p>Connecting to the lab is an optional step, if you don't want to run the demo on your environment. </p> <p>To connecting to the lab platform, you should have the SSH key. The key is being shared during the session. Use the below commands to connect to the lab:  </p> <pre><code>chmod 400 lab.pem\nssh -i \"lab.pem\" -o IdentitiesOnly=yes ubuntu@&lt;IP&gt;\n</code></pre> <p>Instaructure is sharing the screen with tmux. Check the active <code>tmux</code> sessions using the below command:  </p> <pre><code>tmux ls\n</code></pre> <p>A session called <code>lab</code> is ready to attach. You should attach to the session in read-only mode using the below command: </p> <pre><code>tmux attach-session -t lab -r\n</code></pre> <p>To exit the session use <code>Ctr + b + d</code></p>"},{"location":"trainings/docker/nextgenbts.html#install-docker","title":"Install Docker","text":"<p>Docker Engine is available on a variety of Linux distros, macOS, and Windows 10 through Docker Desktop, and as a static binary installation. Find your preferred operating system and follow the structure based on official Docker installation guide.</p> <p>The lab platform is based on Ubuntu version. The following guides show how to install Docker on Ubuntu step-by-step. Ref: Install Docker Engine on Ubuntu</p>"},{"location":"trainings/docker/nextgenbts.html#uninstall-old-versions","title":"Uninstall old versions","text":"<p>Older versions of Docker went by the names of docker, docker.io, or docker-engine, you might also have installations of containerd or runc. Uninstall any such older versions before attempting to install a new version:  </p> <pre><code>for pkg in docker.io docker-doc docker-compose podman-docker containerd runc; do sudo apt-get remove $pkg; done\n</code></pre>"},{"location":"trainings/docker/nextgenbts.html#set-up-the-repository","title":"Set up the repository","text":"<p>Update the apt package index and install packages to allow apt to use a repository over HTTPS:  </p> <pre><code>sudo apt-get update\nsudo apt-get install ca-certificates curl gnupg\n</code></pre> <p>Add Docker\u2019s official GPG key:  </p> <pre><code>sudo install -m 0755 -d /etc/apt/keyrings\ncurl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg\nsudo chmod a+r /etc/apt/keyrings/docker.gpg\n</code></pre> <p>Use the following command to set up the repository:</p> <pre><code>echo \\\n  \"deb [arch=\"$(dpkg --print-architecture)\" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \\\n  \"$(. /etc/os-release &amp;&amp; echo \"$VERSION_CODENAME\")\" stable\" | \\\n  sudo tee /etc/apt/sources.list.d/docker.list &gt; /dev/null\n</code></pre>"},{"location":"trainings/docker/nextgenbts.html#install-docker-engine","title":"Install Docker Engine","text":"<p>Update the apt package index:</p> <pre><code>sudo apt-get update\n</code></pre> <p>Install the latest version of Docker Engine, containerd, and Docker Compose. </p> <pre><code>sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin docker-compose\n</code></pre>"},{"location":"trainings/docker/nextgenbts.html#post-installation-steps","title":"post-installation steps","text":"<p>These optional post-installation procedures shows you how to configure your Linux host machine to work better with Docker. The Docker daemon binds to a Unix socket, not a TCP port. By default it\u2019s the root user that owns the Unix socket, and other users can only access it using sudo. The Docker daemon always runs as the root user.</p> <p>If you don\u2019t want to preface the docker command with sudo, create a Unix group called docker and add users to it. When the Docker daemon starts, it creates a Unix socket accessible by members of the docker group. On some Linux distributions, the system automatically creates this group when installing Docker Engine using a package manager. In that case, there is no need for you to manually create the group.  </p> <p>Create the docker group:</p> <pre><code>sudo groupadd docker\n</code></pre> <p>Add your user to the docker group:</p> <pre><code>sudo usermod -aG docker $USER\n</code></pre> <p>Reboot the system:</p> <pre><code>sudo reboot\n</code></pre> <p>After reboot you should follow the steps to connect to the lab and attach to the session.</p> <p>We can verify the installtion by running:  </p> <pre><code>docker system info\n</code></pre> <p>Which information does the command provide to us?  </p> <p>Docker provides a script to install Docker automatically: get.docker.com Run this command to install Docker automatically: <code>curl -fsSL https://get.docker.com | sh</code></p>"},{"location":"trainings/docker/nextgenbts.html#setup-nextgenbts","title":"Setup NextGenBTS","text":""},{"location":"trainings/docker/nextgenbts.html#downloading-and-setup","title":"Downloading and Setup","text":"<p>To download and set up the NextGenBTS project, follow these steps:  </p> <p>Clone the NextGenBTS Repository:</p> <pre><code>git clone https://github.com/meraj-kashi/NextGenBTS.git\n</code></pre> <p>Navigate to the cloned repository directory:</p> <pre><code>cd NextGenBTS/docker-compose\n</code></pre> <p>Run the script to deploy NextGenBTS application:</p> <pre><code>./run.sh\n</code></pre> <p>Verify the NextGenBTS is up and running by navihating to the below url in your browser: </p> <pre><code>http://&lt;IP&gt;:5000\n</code></pre> <p> </p> <p>Congradulations! You deployed NextGenBTS successfully!</p> <p>You are being provided the usernamer and password to explore the NextGenBTS.</p>"},{"location":"trainings/docker/nextgenbts.html#docker-in-surface","title":"Docker in surface","text":"<p>The below section shows some basic docker commands:  </p> <p>Check the system info again:  </p> <pre><code>docker system info\n</code></pre> <p>Check running containers:  </p> <pre><code>docker ps\n</code></pre> <p>Check local Docker images:  </p> <pre><code>docker image ls\n\nOR\n\ndocker images\n</code></pre> <p>Run the below command to spin up the hello-world container:  </p> <pre><code>docker run hello-world\n</code></pre> <p>Check the output and steps!</p> <p>Now, check available Docker images.  </p> <p>Try to run an Ubuntu Docker container. (Hint: Find the image name from docker hub)  </p> <p>During deployment of Ubuntu container, try to use <code>-i</code> and <code>-t</code> switch. What is the best way to exit the container?  </p> <p>Now, re-run the command and add <code>-d</code> switch. Check number of running containers. </p> <p>Run the below command:  </p> <pre><code>docker ps -a\n</code></pre> <p>Why does it show more containers? Check status of containers!  </p> <p>Try to remove the exited containers using the below command:  </p> <pre><code>docker rm\n</code></pre> <p>Now, try to remove a running container, what is the result of <code>docker rm</code> on a running container?  </p> <p>Before using <code>--force</code> option, try to stop the container. </p> <p>What do you think about deleting Docker images? </p>"},{"location":"trainings/docker/nextgenbts.html#practice-your-knowledge","title":"Practice your knowledge","text":"<p>You can practice these tasks in a terminal or command prompt with Docker installed on your machine. Make sure you have Docker properly installed and configured before starting the practice scenario.</p> <p>Remember to refer to Docker's official documentation for detailed information on each command and its usage. Happy learning!  </p>"},{"location":"trainings/docker/nextgenbts.html#scenario1","title":"Scenario1","text":"<p>Setting up a Web Server Container</p> <p>Task 1: Pull a Docker Image </p> <p>Pull the official Nginx Docker image from the Docker Hub repository using the <code>docker pull</code> command.  </p> <p>Task 2: Run a Container in Interactive Mode </p> <p>Run a new Docker container using the Nginx image with the <code>docker run</code> command in interactive mode (<code>-it</code> flag). Map the container's port <code>80</code> to a port on your local machine (e.g., <code>8080</code>) using the <code>-p</code> flag.  </p> <p>Task 3: Access the Web Server </p> <p>Open a web browser and navigate to <code>http://localhost:8080</code> to access the Nginx web server running inside the Docker container.  </p> <p>Task 4: Attach and Detach from a Running Container </p> <p>While the container is running, detach from the container's console without stopping it using the <code>Ctrl + P, Ctrl + Q</code> key combination.  </p> <p>Task 5: Execute Commands Inside the Container </p> <p>Reattach to the running container's console using the <code>docker attach</code> command. Once inside the container, execute commands like <code>ls</code>, <code>pwd</code>, etc., to explore the container's filesystem. Exit the container's console by typing exit.  </p> <p>Task 6: Copy Files to and from a Container </p> <p>Copy a file from your local machine to the running container using the <code>docker cp</code> command. Copy a file from the container to your local machine using the <code>docker cp</code> command.</p> <p>Task 7: Stop and Start a Container </p> <p>Stop the running container using the <code>docker stop</code> command. Start the stopped container again using the <code>docker start</code> command.  </p> <p>Task 8: Run a Container in Detached Mode </p> <p>Run a new Docker container in detached mode (<code>-d</code> flag) using the Nginx image. Map the container's port <code>80</code> to a port on your local machine (e.g., <code>8081</code>) using the <code>-p</code> flag.  </p>"},{"location":"trainings/docker/nextgenbts.html#scenario2","title":"Scenario2","text":"<p>Docker exec vs. Docker attach </p> <p>Task 1: Run a Container with a Specific Name </p> <ol> <li>Run a new Docker container from the Ubuntu image in detached mode (<code>-d</code> flag).</li> <li>Add a specific name to the container using the <code>--name</code> flag (e.g., \"my-container\").  </li> </ol> <p>Task 2: View Container Processes </p> <ol> <li>Use the <code>docker ps</code> command to view the running containers.</li> <li>Identify the process ID (PID) of the container named \"my-container\".  </li> </ol> <p>Task 3: Execute Commands Inside the Container with <code>docker exec</code> </p> <ol> <li>Use the <code>docker exec</code> command to execute a command (e.g., <code>ls -l</code>) inside the running container named \"my-container\" using the container's PID.</li> <li>Verify that the command is executed successfully and observe the output.  </li> </ol> <p>Task 4: Attach to the Container Console with <code>docker attach</code> </p> <ol> <li>Use the <code>docker attach</code> command to attach to the running container named \"my-container\".</li> <li>Inside the attached console, execute a command (e.g., <code>ls</code>) and observe the output.</li> <li>Detach from the container's console without stopping it using the <code>Ctrl + P, Ctrl + Q</code> key combination.  </li> </ol> <p>Task 5: Delete a Container </p> <ol> <li>Stop the running container named \"my-container\" using the <code>docker stop</code> command.</li> <li>Delete the stopped container using the <code>docker rm</code> command, specifying the container's name or ID.  </li> </ol> <p>Task 6: Delete an Image </p> <ol> <li>List all the Docker images on your system using the <code>docker images</code> command.</li> <li>Identify the ID of the image you want to delete.</li> <li>Delete the image using the <code>docker rmi</code> command, specifying the image's ID.  </li> </ol> <p>Understanding the differences between <code>docker exec</code> and <code>docker attach</code> will help you work with containers more effectively. Additionally, managing containers and images is crucial for maintaining your Docker environment.</p>"},{"location":"trainings/docker/under-the-hood.html","title":"Under the hood of Docker","text":"<p>We have completed the initial warm-up level and have gained some familiarity with Docker at a basic level. Now, it's time to take a deeper dive into Docker and explore the underlying technologies that power it.</p>"},{"location":"trainings/docker/under-the-hood.html#recap","title":"Recap","text":"<p>Let's take a look at a commonly used design that illustrates the differences between virtualization technology and Docker.  </p> <p> </p> <p>Building upon our previous hands-on experience, we observed that container functionality bears a striking resemblance to that of a virtual machine (VM). Try to deploy an Ubuntu container again:  </p> <pre><code>docker run -it ubuntu\n</code></pre> <p>The deployment and execution of Ubuntu were remarkably fast! Run <code>uname -a</code> locally and in container. Compare ouputs!</p> <p>We can observe that the prompt shell has changed to <code>root@&lt;id&gt;</code>, indicating that it is no longer the local prompt of your system. we can check all the running process. There is only one running process which is the current bash with the <code>PID=1</code>.  </p> <pre><code>ps aux\n</code></pre> <p>Run the same command locally. Which command is running with <code>PID=1</code>?   </p> <p>Check the network interfaces:  </p> <pre><code>ip a\n</code></pre> <p>The <code>ip</code> command is not installed by default, so run <code>apt update</code>, and then <code>apt install iproute2</code>.   </p> <p>There are only two ethernet interfaces; <code>LO</code>(Loopback) and <code>eth#</code>. You can see that <code>eth#</code> ip range is different from your local host ip range!  </p> <p>As a final observation, we are installing the Nginx web server and run it in Linux service:  </p> <pre><code>apt update\napt install nginx -y\nservice nginx start\n</code></pre> <p>and check the service status:  </p> <pre><code>service nginx status\n</code></pre> <p>Did you see that we did not use <code>systemctl</code> command to run the Nginx service?  </p> <p>If we deploy an Ubuntu VM on VirtualBox (or any other virtualization platform), we would have almost the same functionalities as the above Ubuntu container.  In a VERY high-level approach, containers can be considered as lightweight VMs (Docker experts are shouting now).  </p>"},{"location":"trainings/docker/under-the-hood.html#low-level-approach","title":"Low-level approach","text":"<p>If you check the hints in the above tests, you can see that a Docker container uses the host kernel and doesn't need <code>init</code> as <code>PID=1</code>. So, although a Docker container's functionality is similar to a VM, a container is just a process on its host.  </p> <p>Run <code>docker ps</code> and find the ubuntu container id. Now, in your local machine, run <code>ps aux | grep &lt;container_id&gt;</code>.  </p> <p>Docker utilizes several Linux functionalities under the hood. In the following section, we will briefly walk through some of those.  </p>"},{"location":"trainings/docker/under-the-hood.html#capabilities","title":"Capabilities","text":"<p>For the purpose of performing permission checks, traditional UNIX implementations distinguish two categories of processes: privileged processes (whose effective user ID is 0, referred to as superuser or root), and unprivileged processes (whose effective UID is nonzero).  Privileged processes bypass all kernel permission checks, while unprivileged processes are subject to full permission checking based on the process's credentials (usually: effective UID, effective GID, and supplementary group list).  </p> <p>Starting with kernel 2.2, Linux divides the privileges traditionally associated with superuser into distinct units, known as capabilities, which can be independently enabled and disabled.  Capabilities are a per-thread attribute. (Ref: Linux man page)  </p> <p>Capabilities apply to both files and threads. File capabilities allow users to execute programs with higher privileges. This is similar to the way the setuid bit works. Thread capabilities keep track of the current state of capabilities in running programs.  </p> <p>In an environment without file based capabilities, it\u2019s not possible for applications to escalate their privileges beyond the bounding set (a set beyond which capabilities cannot grow). Docker sets the bounding set before starting a container. You can use Docker commands to add or remove capabilities to or from the bounding set. By default, Docker drops all capabilities except [those needed] using a whitelist approach. Learn more about Docker and Linux capabilities here.</p>"},{"location":"trainings/docker/under-the-hood.html#cgroups","title":"Cgroups","text":"<p>Control groups, usually referred to as cgroups, are a Linux kernel feature which allow processes to be organized into hierarchical groups whose usage of various types of resources can then be limited and monitored.  The kernel's cgroup interface is provided through a pseudo-filesystem called cgroupfs.  Grouping is implemented in the core cgroup kernel code, while resource tracking and limits are implemented in a set of per-resource-type subsystems (memory, CPU, and so on).  </p> <p>A cgroup is a collection of processes that are bound to a set of limits or parameters defined via the cgroup filesystem. A subsystem is a kernel component that modifies the behavior of the processes in a cgroup.  Various subsystems have been implemented, making it possible to do things such as limiting the amount of CPU time and memory available to a cgroup, accounting for the CPU time used by a cgroup, and freezing and resuming execution of the processes in a cgroup.  Subsystems are sometimes also known as resource controllers (or simply, controllers).  </p> <p>The cgroups for a controller are arranged in a hierarchy.  This hierarchy is defined by creating, removing, and renaming subdirectories within the cgroup filesystem.  At each level of the hierarchy, attributes (e.g., limits) can be defined.  The limits, control, and accounting provided by cgroups generally have effect throughout the subhierarchy underneath the cgroup where the attributes are defined.  Thus, for example, the limits placed on a cgroup at a higher level in the hierarchy cannot be exceeded by descendant cgroups.  (Ref: Linux man page)  </p> <pre><code>docker ps\nps aux | grep &lt;container-id&gt;\ncat /proc/[pid]/cgroup\n</code></pre> <p>For each cgroup hierarchy of which the process is a member, there is one entry containing three colon-separated fields:</p> <p>hierarchy-ID::cgroup-path</p> <ul> <li>hierarchy-ID: For the cgroups version 2 hierarchy, this field contains the value 0. </li> <li>cgroup-path: This field contains the pathname of the control group in the hierarchy to which the process belongs.  This pathname is relative to the mount point of the hierarchy.</li> </ul> <pre><code>docker info | grep containerd\n</code></pre> <p>More info about containerd: https://containerd.io/</p> <p> </p> <p>more details about Linux cgroups:</p> <pre><code>systemd-cgls\n</code></pre> <pre><code>systemd-cgtop\n</code></pre> <pre><code>docker stat \n</code></pre> <p>Learn more about Docker and Linux Cgroups here.</p>"},{"location":"trainings/docker/under-the-hood.html#namespaces","title":"Namespaces","text":"<p>Namespaces are a feature of the Linux kernel that partitions kernel resources such that one set of processes sees one set of resources while another set of processes sees a different set of resources.  </p> <p>Starting from kernel version 5.6 onwards, there exist eight different types of namespaces. Regardless of the type, the functionality of namespaces remains consistent. Each process is linked to a specific namespace, restricting its access to only the resources associated with that particular namespace and any relevant descendant namespaces. As a result, every process or group of processes can have a distinct perspective on the available resources. The specific resource that is isolated depends on the type of namespace created for a particular process group. The below list shows different type of namespaces:  </p> <ul> <li>Mount (mnt)  </li> <li>Process ID (pid)  </li> <li>Network (net)  </li> <li>Inter-process Communication (ipc)  </li> <li>UNIX Time-Sharing (UTS)  </li> <li>User ID (user)  </li> <li>Control group (cgroup)  </li> <li>Time Namespace  </li> </ul> <p>To see the namespace example, pull the NextGenBTS project: (<code>https://github.com/meraj-kashi/NextGenBTS</code>), navigate to <code>dev/namespace/network</code>. Build the project:  </p> <pre><code>gcc -o network_namespace network_namespace.c\n</code></pre> <p>run the code and check ethernet interfaces:  </p> <pre><code>./network_namespace\n</code></pre>"},{"location":"trainings/docker/under-the-hood.html#seccom","title":"Seccom","text":"<p>Seccomp, short for Secure Computing Mode, is a feature in the Linux kernel that enhances multiple security aspects, providing a more secure environment for running Docker. More information here.  </p>"},{"location":"trainings/docker/under-the-hood.html#apparmor","title":"AppArmor","text":"<p>AppArmor is a security module integrated into the Linux kernel, which enables system administrators to impose restrictions on the capabilities of programs through per-program profiles. These profiles define what actions a program is allowed to perform, such as granting network access, raw socket access, and read, write, or execute permissions for specific file paths. Check this blog for more information.</p>"},{"location":"trainings/docker/under-the-hood.html#demo","title":"Demo","text":"<pre><code>mount --make-rprivate / \nmkdir -p images containers\nbtrfs subvol create images/alpine\nwget https://dl-cdn.alpinelinux.org/alpine/v3.18/releases/x86_64/alpine-minirootfs-3.18.0-x86_64.tar.gz\ntar -C images/alpine/ -xf alpine-minirootfs-3.18.0-x86_64.tar.gz\nbtrfs subvol snapshot images/alpine/ containers/nextgenbts\nchroot containers/nextgenbts/ sh\nexit\nunshare --mount --uts --ipc --net --pid --fork bash\nhostname nextgenbts\nexec bash\nps\npidof unshare\nkill &lt;pid&gt;\nmount -t proc none /proc\nps aux\nip a\n</code></pre>"}]}